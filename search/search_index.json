{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AWS Observability Accelerator for CDK","text":"<p>Welcome to the <code>AWS Observability Accelerator for CDK</code>!</p> <p>The AWS Observability Accelerator for CDK is a set of opinionated modules to help you set up observability for your AWS environments with AWS Native services and AWS-managed observability services such as Amazon Managed Service for Prometheus,Amazon Managed Grafana, AWS Distro for OpenTelemetry (ADOT) and Amazon CloudWatch.</p> <p>One of the fallacies of distributed computing is that observability is optional. It's a prevalent but wrong assumption. To have a reliable and changeable system, you need to have proper observability in place. And this project aims to help you.</p> <p>AWS Observability Accelerator for CDK provides patterns with:</p> <ul> <li> ADOT Collector monitoring</li> <li> Amazon Cloudwatch dashboards</li> <li> Amazon Managed Grafana Dashboards</li> <li> Amazon Managed Service for Prometheus - Alerting rules</li> <li> Amazon Managed Service for Prometheus - Recording rules</li> <li> Cost monitoring</li> <li> Curated metrics with CloudWatch Container Insights</li> <li> Curated metrics with ADOT and Amazon Service for Prometheus Exporter</li> <li> GPU Infrastructure and Workload monitoring</li> <li> Inferentia Infrastructure and Workload monitoring</li> <li> Istio Service Mesh monitoring</li> <li> NGINX monitoring</li> <li> Java/JMX Workload monitoring</li> <li> Logs using FluentBit and ADOT Exporter</li> <li> Traces collection with ADOT XRAY Exporter</li> <li> Traces collection with XRAY Daemon</li> </ul>"},{"location":"#single-eks-cluster-aws-native-observability-accelerator","title":"Single EKS Cluster AWS Native Observability Accelerator","text":""},{"location":"#single-eks-cluster-open-source-observability-accelerator","title":"Single EKS Cluster Open Source Observability Accelerator","text":""},{"location":"#patterns","title":"Patterns","text":"<p>The individual patterns can be found in the <code>lib</code> directory.  Most of the patterns are self-explanatory, for some more complex examples please use this guide and docs/patterns directory for more information.</p>"},{"location":"#usage","title":"Usage","text":"<p>Before proceeding, make sure AWS CLI is installed on your machine.</p> <p>To use this solution, you must have Node.js and npm installed. You will also use <code>make</code> and <code>brew</code> to simplify build and other common actions.</p>"},{"location":"#workstation-setup-options","title":"Workstation Setup Options","text":""},{"location":"#devcontainer-setup","title":"DevContainer Setup","text":"<p>Users can choose this option, if you dont want to run this solution on a mac or ubuntu machine. Please use the dev container configuration in the <code>.devcontainer</code> folder with devpod or any other dev container environment to create a development environment with dependencies such as Node, NPM, aws-cli, aws-cdk, kubectl, helm dependencies for your local development with <code>cdk-aws-observability-accelerator</code> solution. </p>"},{"location":"#ubuntu-setup","title":"Ubuntu Setup","text":"<p>Follow the below steps to setup and leverage cdk-aws-observability-accelerator in your Ubuntu Linux machine.</p> <ol> <li>Update the package list</li> </ol> <p>Update the package list to ensure you're installing the latest versions.</p> <pre><code>sudo apt update\n</code></pre> <ol> <li>Install make</li> </ol> <pre><code>sudo apt install make\n</code></pre> <ol> <li>Install Node.js and npm</li> </ol> <p>Install Node.js and npm using the NodeSource binary distributions.</p> <pre><code>curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash - &amp;&amp;\\\nsudo apt-get install -y nodejs\n</code></pre> <p>Note: The Node.js package from NodeSource includes npm</p> <ol> <li>Verify Node.js and npm Installation</li> </ol> <p>Check the installed version of Node.js:</p> <pre><code>node -v\n</code></pre> <p>The output should be <code>v20.x.x</code>.</p> <p>Check the installed version of npm:</p> <pre><code>npm -v\n</code></pre> <p>The output should be a version greater than <code>10.1.x</code>.</p> <p>If your npm version is not <code>10.1.x</code> or above, update npm with the following command:</p> <pre><code>sudo npm install -g npm@latest\n</code></pre> <p>Verify the installed version by running <code>npm -v</code>.</p> <ol> <li>Install brew on ubuntu by following instructions as detailed in docs.brew.sh</li> </ol> <pre><code> /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>Add Homebrew to your PATH</p> <pre><code>test -d ~/.linuxbrew &amp;&amp; eval \"$(~/.linuxbrew/bin/brew shellenv)\"\ntest -d /home/linuxbrew/.linuxbrew &amp;&amp; eval \"$(/home/linuxbrew/.linux  brew/bin/brew shellenv)\"\ntest -r ~/.bash_profile &amp;&amp; echo \"eval \\\"\\$($(brew --prefix)/bin/brew shellenv)\\\"\" &gt;&gt; ~/.bash_profile\necho \"eval \\\"\\$($(brew --prefix)/bin/brew shellenv)\\\"\" &gt;&gt; ~/.profile\n</code></pre> <p>Post completing the above, continue from Step: Repo setup</p>"},{"location":"#mac-setup","title":"Mac Setup:","text":"<p>Follow the below steps to setup and leverage <code>cdk-aws-observability-accelerator</code> in your local Mac laptop.</p> <ol> <li>Install <code>make</code> and <code>node</code> using brew</li> </ol> <pre><code>brew install make\nbrew install node\n</code></pre> <ol> <li>Install <code>npm</code></li> </ol> <pre><code>sudo npm install -g n\nsudo n stable\n</code></pre> <ol> <li> <p>Make sure the following pre-requisites are met:</p> </li> <li> <p>Node version is a current stable node version 20.x.x</p> </li> </ol> <pre><code>$ node -v\nv20.8.0\n</code></pre> <p>Update (provided Node version manager is installed): <code>n stable</code>. May require <code>sudo</code>.</p> <ul> <li>NPM version must be 10.1 or above:</li> </ul> <pre><code>$ npm -v\n10.1.0\n</code></pre> <p>Updating npm: <code>sudo n stable</code> where stable can also be a specific version above 10.1. May require <code>sudo</code>.</p>"},{"location":"#repo-setup","title":"Repo setup","text":"<ol> <li>Clone the <code>cdk-aws-observability-accelerator</code> repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <p>PS: If you are contributing to this repo, please make sure to fork the repo, add your changes and create a PR against it.</p> <ol> <li> <p>Once you have cloned the repo, you can open it using your favourite IDE and run the below commands to install the dependencies and build the existing patterns.</p> </li> <li> <p>Install project dependencies.</p> </li> </ol> <pre><code>make deps\n</code></pre> <ul> <li>To view patterns that are available to be deployed, execute the following:</li> </ul> <pre><code>make build\n</code></pre> <ul> <li>To list the existing CDK AWS Observability Accelerator Patterns</li> </ul> <pre><code>make list\n</code></pre> <p>Note: Some patterns have a hard dependency on AWS Secrets (for example GitHub access tokens). Initially you will see errors complaining about lack of the required secrets. It is normal. At the bottom, it will show the list of patterns which can be deployed, in case the pattern you are looking for is not available, it is due to the hard dependency which can be fixed by following the docs specific to those patterns.</p> <pre><code>To work with patterns use:\n    $ make pattern &lt;pattern-name&gt; &lt;list | deploy | synth | destroy&gt;\nExample:\n    $ make pattern single-new-eks-opensource-observability deploy\n\nPatterns: \n\n        existing-eks-awsnative-observability\n        existing-eks-mixed-observability\n        existing-eks-opensource-observability\n        multi-acc-new-eks-mixed-observability\n        single-new-eks-awsnative-fargate-observability\n        single-new-eks-awsnative-observability\n        single-new-eks-cluster\n        single-new-eks-cost-monitoring\n        single-new-eks-fargate-opensource-observability\n        single-new-eks-gpu-opensource-observability\n        single-new-eks-graviton-opensource-observability\n        single-new-eks-inferentia-opensource-observability\n        single-new-eks-mixed-observability\n        single-new-eks-opensource-observability\n</code></pre> <ul> <li>Bootstrap your CDK environment.</li> </ul> <pre><code>npx cdk bootstrap\n</code></pre> <ul> <li>You can then deploy a specific pattern with the following:</li> </ul> <pre><code>make pattern single-new-eks-opensource-observability deploy\n</code></pre> <ul> <li>To access instructions for individual patterns check documentation in <code>docs/patterns</code> directory.</li> </ul>"},{"location":"#developer-flow","title":"Developer Flow","text":""},{"location":"#modifications","title":"Modifications","text":"<p>All files are compiled to the dist folder including <code>lib</code> and <code>bin</code> directories. For iterative development (e.g. if you make a change to any of the patterns) make sure to run compile:</p> <pre><code>make compile\n</code></pre> <p>The <code>compile</code> command is optimized to build only modified files and is fast.</p>"},{"location":"#new-patterns","title":"New Patterns","text":"<p>To create a new pattern, please follow these steps:</p> <ol> <li>Under lib create a folder for your pattern, such as <code>&lt;pattern-name&gt;-pattern</code>. If you plan to create a set of patterns that represent a particular subdomain, e.g. <code>security</code> or <code>hardening</code>, please create an issue to discuss it first. If approved, you will be able to create a folder with your subdomain name and group your pattern constructs under it.</li> <li>Blueprints generally don't require a specific class, however we use a convention of wrapping each pattern in a plain class like <code>&lt;Pattern-Name&gt;Pattern</code>. This class is generally placed in <code>index.ts</code> under your pattern folder.</li> <li>Once the pattern implementation is ready, you need to include it in the list of the patterns by creating a file <code>bin/&lt;pattern-name&gt;.ts</code>. The implementation of this file is very light, and it is done to allow patterns to run independently.</li> </ol> <p>Example simple synchronous pattern:</p> <pre><code>import SingleNewEksOpenSourceobservabilityPattern from '../lib/single-new-eks-opensource-observability-pattern';\nimport { configureApp } from '../lib/common/construct-utils';\n\nconst app = configureApp();\n\nnew SingleNewEksOpenSourceobservabilityPattern(app, 'single-new-eks-opensource');\n // configureApp() will create app and configure loggers and perform other prep steps\n</code></pre>"},{"location":"#security","title":"Security","text":"<p>See CONTRIBUTING for more information.</p>"},{"location":"#license","title":"License","text":"<p>This library is licensed under the MIT-0 License. See the LICENSE file.</p>"},{"location":"contributors/","title":"Contributors","text":"<p>The content on this site is maintained by the Solutions Architects from the AWS observability team with support from the AWS service teams and other volunteers from across the organization.</p> <p>Our goal is to make it easier to use AWS Native and Open Source Observability Services.</p> <p>The core team include the following people:</p> <ul> <li>Elamaran Shanmugam</li> <li>Imaya Kumar Jagannathan</li> <li>Kevin Lewin</li> <li>Michael Hausenblas</li> <li>Mikhail Shapirov</li> <li>Rodrigue Koffi</li> </ul> <p>We welcome the wider open source community and thank those who contribute to this project.</p> <p>Note that all information published on this site is available via the Apache 2.0 license.</p>"},{"location":"logs/","title":"Viewing Logs","text":"<p>By default, we deploy a FluentBit daemon set in the cluster to collect worker logs for all namespaces. Logs are collected and exported to Amazon CloudWatch Logs, which enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service.</p> <p>Further configuration options are available in the module documentation. This guide shows how you can leverage either CloudWatch Logs or Amazon Managed Grafana for your cluster and application logs.</p>"},{"location":"logs/#viewing-logs-in-cloudwatch-logs-insights","title":"Viewing Logs in CloudWatch Logs Insights","text":"<p>Navigate to CloudWatch, then go to \"Logs Insights\"</p> <p>In the dropdown, select any of the logs that begin with \"/aws/eks/single-new-eks-mixed-observability-accelerator\" and run a query.</p> <p>Example with \"kubesystem\" log group:</p> <p></p> <p>Then you can view the results of your query:</p> <p></p> <p>You can also find control plane logs in CloudWatch with Control Plane logs visible under the <code>/cluster</code> log group:</p> <p></p> <p>You can then filter by patterns within the logs to find specific events like so: </p>"},{"location":"logs/#viewing-logs-in-grafana","title":"Viewing Logs in Grafana","text":""},{"location":"logs/#using-cloudwatch-logs-as-data-source-in-grafana","title":"Using CloudWatch Logs as data source in Grafana","text":"<p>Follow the documentation to enable Amazon CloudWatch as a data source. Make sure to provide permissions.</p> <p>All logs are delivered in the following CloudWatch Log groups naming pattern: <code>/aws/eks/$PATTERN</code>. Log streams follow <code>{container-name}.{pod-name}</code>. In Grafana, querying and analyzing logs is done with CloudWatch Logs Insights</p>"},{"location":"logs/#example-adot-collector-logs","title":"Example - ADOT collector logs","text":"<p>Select one or many log groups and run the following query. The example below, queries AWS Distro for OpenTelemetry (ADOT) logs</p> <pre><code>fields @timestamp, log\n| order @timestamp desc\n| limit 100\n</code></pre> <p></p>"},{"location":"logs/#example-using-time-series-visualizations","title":"Example - Using time series visualizations","text":"<p>CloudWatch Logs syntax provide powerful functions to extract data from your logs. The <code>stats()</code> function allows you to calculate aggregate statistics with log field values. This is useful to have visualization on non-metric data from your applications.</p> <p>In the example below, we use the following query to graph the number of metrics collected by the ADOT collector</p> <pre><code>fields @timestamp, log\n| parse log /\"#metrics\": (?&lt;metrics_count&gt;\\d+)}/\n| stats avg(metrics_count) by bin(5m)\n| limit 100\n</code></pre> <p>Tip</p> <p>You can add logs in your dashboards with logs panel types or time series depending on your query results type.</p> <p></p> <p>Warning</p> <p>Querying CloudWatch logs will incur costs per GB scanned. Use small time windows and limits in your queries. Checkout the CloudWatch pricing page for more info.</p>"},{"location":"support/","title":"Support &amp; Feedback","text":"<p>AWS Observability Accelerator for CDK is maintained by AWS Solution Architects. It is not part of an AWS service and support is provided best-effort by the AWS Observability Accelerator community.</p> <p>To post feedback, submit feature ideas, or report bugs, please use the issues section of this GitHub repo.</p> <p>If you are interested in contributing, see the contribution guide.</p>"},{"location":"tracing/","title":"Tracing on Amazon EKS","text":"<p>Distributed tracing helps you have end-to-end visibility between transactions in distributed nodes. The <code>eks-monitoring</code> module is configured  by default to collect traces into AWS X-Ray.</p> <p>The AWS Distro for OpenTelemetry collector is configured to receive traces in the OTLP format (OTLP receiver), using the OpenTelemetry SDK or auto-instrumentation agents.</p> <p>Note</p> <p>To disable the tracing configuration, <code>XrayAdotAddOn</code> for Mixed and Open Source Observability Accelerators from the CDK Observability Patterns</p>"},{"location":"tracing/#instrumentation","title":"Instrumentation","text":"<p>Let's take a sample application that is already instrumented with the OpenTelemetry SDK.</p> <p>Note</p> <p>To learn more about instrumenting with OpenTelemetry, please visit the OpenTelemetry documentation for your programming language.</p> <p>Cloning the repo</p> <pre><code>git clone https://github.com/aws-observability/aws-otel-community.git\ncd aws-otel-community/sample-apps/go-sample-app\n</code></pre> <p>Highlighting code sections</p>"},{"location":"tracing/#deploying-on-amazon-eks","title":"Deploying on Amazon EKS","text":"<p>Using the sample application, we will build a container image, create and push an image to Amazon ECR. We will use the go-sample-app Kubernetes manifest to deploy to EKS cluster.</p> <p>Warning</p> <p>The following steps require that you have an EKS cluster ready. To deploy an EKS cluster, please visit our example.</p>"},{"location":"tracing/#building-container-image","title":"Building container image","text":"amd64 linuxcross platform build <pre><code>docker build -t go-sample-app .\n</code></pre> <pre><code>docker buildx build -t go-sample-app . --platform=linux/amd64\n</code></pre>"},{"location":"tracing/#publishing-on-amazon-ecr","title":"Publishing on Amazon ECR","text":"using docker <pre><code>export ECR_REPOSITORY_URI=$(aws ecr create-repository --repository go-sample-app --query repository.repositoryUri --output text)\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_REPOSITORY_URI\ndocker tag go-sample-app:latest \"${ECR_REPOSITORY_URI}:latest\"\ndocker push \"${ECR_REPOSITORY_URI}:latest\"\n</code></pre>"},{"location":"tracing/#deploying-on-amazon-eks_1","title":"Deploying on Amazon EKS","text":"eks.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: go-sample-app\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: go-sample-app\n  template:\n    metadata:\n      labels:\n        app: go-sample-app\n    spec:\n      containers:\n        - name: go-sample-app\n          image: \"${ECR_REPOSITORY_URI}:latest\" # make sure to replace this variable\n          imagePullPolicy: Always\n          env:\n          - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\n            value: 'http://adot-collector.adot-collector-kubeprometheus.svc.cluster.local:4317'\n          resources:\n            limits:\n              cpu:  300m\n              memory: 300Mi\n            requests:\n              cpu: 100m\n              memory: 180Mi\n          ports:\n            - containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: go-sample-app\n  namespace: default\n  labels:\n    app: go-sample-app\nspec:\n  ports:\n    - protocol: TCP\n      port: 8080\n      targetPort: 8080\n  selector:\n    app: go-sample-app\n</code></pre>"},{"location":"tracing/#deploying-and-testing","title":"Deploying and testing","text":"<p>With the Kubernetes manifest ready, run:</p> <pre><code>kubectl apply -f eks.yaml\n</code></pre> <p>You should see the pods running with the command:</p> <pre><code>kubectl get pods\nNAME                              READY   STATUS    RESTARTS        AGE\ngo-sample-app-67c48ff8c6-bdw74    1/1     Running   0               4s\ngo-sample-app-67c48ff8c6-t6k2j    1/1     Running   0               4s\n</code></pre> <p>To simulate some traffic you can forward the service port to your local host and test a few queries</p> <pre><code>kubectl port-forward deployment/go-sample-app 8080:8080\n</code></pre> <p>Test a few endpoints</p> <pre><code>curl http://localhost:8080/\ncurl http://localhost:8080/outgoing-http-call\ncurl http://localhost:8080/aws-sdk-call\ncurl http://localhost:8080/outgoing-sampleapp\n</code></pre>"},{"location":"tracing/#visualizing-traces","title":"Visualizing traces","text":"<p>As this is a basic example, the service map doesn't have a lot of nodes, but this shows you how to setup tracing in your application and deploying it on Amazon EKS using our OSS observability patterns.</p> <p>With Flux and Grafana Operator, the OSS pattern configures an AWS X-Ray data source on your provided Grafana workspace. Open the Grafana explorer view and select the X-Ray data source. If you type the query below, and select <code>Trace List</code> for Query Type, you should see the list of traces occured in the selected timeframe.</p> <p></p> <p>You can add the service map to a dashboard, for example a service focused dashboard. You can click on any of the traces to view a node map and the traces details.</p> <p>There is a button that can take you the CloudWatch console to view the same data. If your logs are stored on CloudWatch Logs, this page can present all the logs in the trace details page. The CloudWatch Log Group name should be added to the trace as an attribute. Read more about this in our One Observability Workshop</p> <p></p>"},{"location":"tracing/#resources","title":"Resources","text":"<ul> <li>AWS Observability Best Practices</li> <li>One Observability Workshop</li> <li>AWS Distro for OpenTelemetry documentation</li> <li>AWS X-Ray user guide</li> <li>OpenTelemetry documentation</li> </ul>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-adotmetrics-collection-observability/","title":"Single Cluster Open Source Observability - OTEL Collector Monitoring","text":""},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-adotmetrics-collection-observability/#objective","title":"Objective","text":"<p>This pattern aims to add Observability on top of an existing EKS cluster and adds monitoring for ADOT collector health, with open source managed AWS services.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-adotmetrics-collection-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-adotmetrics-collection-observability/#deploying","title":"Deploying","text":"<ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>Warning</p> <p>Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens.</li> </ol> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository.</p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>   \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_ADOTHEALTH_DASH_URL\": \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/adot/adothealth.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/adot\"\n        }\n      ]\n    },\n    \"adotcollectormetrics.pattern.enabled\": true\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-opensource-observability deploy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-adotmetrics-collection-observability/#visualization","title":"Visualization","text":"<p>The OpenTelemetry collector produces metrics to monitor the entire pipeline. </p> <p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see three new dashboard named <code>OpenTelemetry Health Collector</code>, under <code>Observability Accelerator Dashboards</code></p> <p>This dashboard shows useful telemetry information about the ADOT collector itself which can be helpful when you want to troubleshoot any issues with the collector or understand how much resources the collector is consuming.</p> <p>Below diagram shows an example data flow and the components in an ADOT collector:</p> <p></p> <p>In this dashboard, there are five sections. Each section has metrics relevant to the various components of the AWS Distro for OpenTelemetry (ADOT) collector :</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-adotmetrics-collection-observability/#receivers","title":"Receivers","text":"<p>Shows the receiver\u2019s accepted and refused rate/count of spans and metric points that are pushed into the telemetry pipeline.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-adotmetrics-collection-observability/#processors","title":"Processors","text":"<p>Shows the accepted and refused rate/count of spans and metric points pushed into next component in the pipeline. The batch metrics can help to understand how often metrics are sent to exporter and the batch size.</p> <p></p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-adotmetrics-collection-observability/#exporters","title":"Exporters","text":"<p>Shows the exporter\u2019s accepted and refused rate/count of spans and metric points that are pushed to any of the destinations. It also shows the size and capacity of the retry queue. These metrics can be used to understand if the collector is having issues in sending trace or metric data to the destination configured.</p> <p></p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-adotmetrics-collection-observability/#collectors","title":"Collectors","text":"<p>Shows the collector\u2019s operational metrics (Memory, CPU, uptime). This can be used to understand how much resources the collector is consuming.</p> <p></p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-adotmetrics-collection-observability/#data-flow","title":"Data Flow","text":"<p>Shows the metrics and spans data flow through the collector\u2019s components.</p> <p></p> <p>Note:     To read more about the metrics and the dashboard used, visit the upstream documentation here.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-adotmetrics-collection-observability/#disable-adot-health-monitoring","title":"Disable ADOT health monitoring","text":"<p>Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>   \"context\": {\n    \"adotcollectormetrics.pattern.enabled\": false\n  }\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-adotmetrics-collection-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-opensource-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-apiserver-observability/","title":"Single Cluster Open Source Observability - API Server Monitoring","text":""},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-apiserver-observability/#objective","title":"Objective","text":"<p>This pattern aims to add Observability on top of an existing EKS cluster and adds API server monitoring, with open source managed AWS services.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-apiserver-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-apiserver-observability/#deploying","title":"Deploying","text":"<p>!!! note If control plane logging is not enabled in the existing cluster, edit  <code>lib/existing-eks-opensource-observability-pattern/index.ts</code> to include <code>.enableControlPlaneLogging()</code> as shown below: <pre><code>    ObservabilityBuilder.builder()\n        // some properties\n        .enableControlPlaneLogging()\n        // other properties\n        .build(scope, stackId);\n</code></pre></p> <ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>Warning</p> <p>Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens.</li> </ol> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository.</p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_APISERVER_BASIC_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-basic.json\",\n        \"GRAFANA_APISERVER_ADVANCED_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-advanced.json\",\n        \"GRAFANA_APISERVER_TROUBLESHOOTING_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-troubleshooting.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/apiserver\"\n        }\n      ]\n    },\n    \"apiserver.pattern.enabled\": true\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-opensource-observability deploy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-apiserver-observability/#visualization","title":"Visualization","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see three new dashboard named <code>Kubernetes/Kube-apiserver (basic), Kubernetes/Kube-apiserver (advanced), Kubernetes/Kube-apiserver (troubleshooting)</code>, under <code>Observability Accelerator Dashboards</code>:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (basic)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (advanced)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (troubleshooting)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-apiserver-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Please see Single New Nginx Observability Accelerator.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-apiserver-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-opensource-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/","title":"Single Cluster AWS Native Observability","text":""},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Existing EKS Cluster AWS Native Observability pattern, using AWS native tools such as CloudWatch and Logs and Open Source tools such as AWS Distro for OpenTelemetry (ADOT).</p> <p>[!NOTE] Currently, Xray AddOn is not supported for imported clusters. The Xray AddOn requires access to the nodegroup which is not available with the imported cluster and it does not support IRSA as of now and requires modification of the node instance role. Once it supports IRSA, we would update this pattern to work with the existing clusters.</p> <p></p> <p>This example makes use of CloudWatch, as a metric and log aggregation layer. In order to collect the metrics and traces, we use the Open Source ADOT collector. Fluent Bit is used to export the logs to CloudWatch Logs.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/#objective","title":"Objective","text":"<p>This pattern aims to add Observability on top of an existing EKS cluster, with AWS services.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/#deploying","title":"Deploying","text":"<p>!!! note If control plane logging is not enabled in the existing cluster, edit  <code>lib/existing-eks-awsnative-observability-pattern/index.ts</code> to include <code>.enableControlPlaneLogging()</code> as shown below: <pre><code>    ObservabilityBuilder.builder()\n        // some properties\n        .enableControlPlaneLogging()\n        // other properties\n        .build(scope, stackId);\n</code></pre></p> <ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-awsnative-observability deploy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message. <pre><code>aws eks update-kubeconfig --name single-new-eks-observability-accelerator --region &lt;your-region&gt; --role-arn arn:aws:iam::**************:role/single-new-eks-observabil-singleneweksobservabilit-CPAN247ASDF\n</code></pre> Let\u2019s verify the resources created by steps above.</p> <pre><code>kubectl get nodes -o wide\n</code></pre> <p>Output: <pre><code>NAME                           STATUS   ROLES    AGE   VERSION\nip-10-0-145-216.ec2.internal   Ready    &lt;none&gt;   14m   v1.25.11-eks-a5565ad\n</code></pre></p> <p>Next, lets verify the namespaces in the cluster: <pre><code>kubectl get ns # Output shows all namespace\n</code></pre></p> <p>Output: <pre><code>NAME                       STATUS   AGE\namazon-cloudwatch          Active   5h36m\ncert-manager               Active   5h36m\ndefault                    Active   5h46m\nkube-node-lease            Active   5h46m\nkube-public                Active   5h46m\nkube-system                Active   5h46m\nprometheus-node-exporter   Active   5h36m\n</code></pre></p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/#visualization","title":"Visualization","text":"<ul> <li>Navigate to CloudWatch &gt; Insights &gt; Container Insights and select cluster, select <code>single-new-eks-cluster</code> if you created cluster with pattern mentioned from above guide, otherwise select relevant cluster.</li> <li>Now select <code>amazon-metrics</code> namepsace </li> <li> <p>On a same view, select 'EKS Pods', which provides insights overview of all the pods as shown below </p> </li> <li> <p>Refer to \"Using CloudWatch Logs Insights to Query Logs in Logging.</p> </li> </ul>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-awsnative-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-awsnative-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-mixed-observability/","title":"Single Cluster AWS Mixed Observability","text":""},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-mixed-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Existing EKS Cluster AWS Mixed Observability pattern, using AWS native tools such as CloudWatch and X-Ray and Open Source tools such as AWS Distro for OpenTelemetry (ADOT) and Prometheus Node Exporter.</p> <p></p> <p>This example makes use of CloudWatch, as a metric and log aggregation layer, while X-Ray is used as a trace-aggregation layer. In order to collect the metrics and traces, we use the Open Source ADOT collector. Fluent Bit is used to export the logs to CloudWatch Logs.</p> <p>In this architecture, AWS X-Ray provides a complete view of requests as they travel through your application and filters visual data across payloads, functions, traces, services, and APIs. X-Ray also allows you to perform analytics, to gain powerful insights about your distributed trace data.</p> <p>Utilizing CloudWatch and X-Ray as an aggregation layer allows for a fully-managed scalable telemetry backend. In this example we get those benefits while still having the flexibility and rapid development of the Open Source collection tools.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-mixed-observability/#objective","title":"Objective","text":"<p>This pattern aims to add Observability on top of an existing EKS cluster, with a mixture of AWS native and open source managed AWS services.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-mixed-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-mixed-observability/#deploying","title":"Deploying","text":"<p>!!! note If control plane logging is not enabled in the existing cluster, edit <code>lib/existing-eks-mixed-observability-pattern/index.ts</code> to include <code>.enableControlPlaneLogging()</code> as shown below: <pre><code>    ObservabilityBuilder.builder()\n        // some properties\n        .enableControlPlaneLogging()\n        // other properties\n        .build(scope, stackId);\n</code></pre></p> <ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-mixed-observability deploy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-mixed-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Please see Single New EKS Cluster AWS Mixed Observability Accelerator.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-mixed-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-mixed-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/","title":"Single Cluster Open Source Observability - NGINX Monitoring","text":""},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Existing EKS Cluster NGINX pattern, using Open Source tools such as AWS Distro for OpenTelemetry (ADOT), Amazon Managed Grafana workspace and Prometheus.</p> <p>The current example deploys the AWS Distro for OpenTelemetry Operator for Amazon EKS with its requirements and make use of an existing Amazon Managed Grafana workspace. It creates a new Amazon Managed Service for Prometheus workspace. And You will gain both visibility on the cluster and NGINX based applications.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#objective","title":"Objective","text":"<p>This pattern aims to add Observability on top of an existing EKS cluster and NGINX workloads, with open source managed AWS services.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#deploying","title":"Deploying","text":"<p>!!! note If control plane logging is not enabled in the existing cluster, edit  <code>lib/existing-eks-opensource-observability-pattern/index.ts</code> to include <code>.enableControlPlaneLogging()</code> as shown below: <pre><code>    ObservabilityBuilder.builder()\n        // some properties\n        .enableControlPlaneLogging()\n        // other properties\n        .build(scope, stackId);\n</code></pre></p> <ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>Warning</p> <p>Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens.</li> </ol> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository.</p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_NGINX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/nginx/nginx.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/nginx\"\n        }\n      ]\n    },\n    \"nginx.pattern.enabled\": true\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-opensource-observability deploy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#deploy-an-example-nginx-application","title":"Deploy an example Nginx application","text":"<p>In this section we will deploy sample application and extract metrics using AWS OpenTelemetry collector.</p> <ol> <li> <p>Add NGINX ingress controller add-on into lib/existing-eks-opensource-observability-pattern/index.ts in add-on array. <pre><code>        const addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n            new blueprints.addons.CloudWatchLogsAddon({\n                logGroupPrefix: `/aws/eks/${stackId}`,\n                logRetentionDays: 30\n            }),\n            new blueprints.addons.XrayAdotAddOn(),\n            new blueprints.addons.FluxCDAddOn({\"repositories\": [fluxRepository]}),\n            new GrafanaOperatorSecretAddon(),\n            new blueprints.addons.NginxAddOn({\n                name: \"ingress-nginx\",\n                chart: \"ingress-nginx\",\n                repository: \"https://kubernetes.github.io/ingress-nginx\",\n                version: \"4.7.2\",\n                namespace: \"nginx-ingress-sample\",\n                values: {\n                    controller: { \n                        metrics: {\n                            enabled: true,\n                            service: {\n                                annotations: {\n                                    \"prometheus.io/port\": \"10254\",\n                                    \"prometheus.io/scrape\": \"true\"\n                                }\n                            }\n                        }\n                    }\n                }\n            }),\n        ];\n</code></pre></p> </li> <li> <p>Deploy pattern again  <pre><code>make pattern existing-eks-opensource-observability deploy\n</code></pre></p> </li> <li> <p>Verify if the application is running <pre><code>kubectl get pods -n nginx-ingress-sample\n</code></pre></p> </li> <li> <p>Set an EXTERNAL-IP variable to the value of the EXTERNAL-IP column in the row of the NGINX ingress controller. <pre><code>EXTERNAL_IP=$(kubectl get svc blueprints-addon-nginx-ingress-nginx-controller -n nginx-ingress-sample --output jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n</code></pre></p> </li> <li> <p>Start some sample NGINX traffic by entering the following command. <pre><code>SAMPLE_TRAFFIC_NAMESPACE=nginx-sample-traffic\ncurl https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/k8s-deployment-manifest-templates/nginx/nginx-traffic-sample.yaml |\nsed \"s/{{external_ip}}/$EXTERNAL_IP/g\" |\nsed \"s/{{namespace}}/$SAMPLE_TRAFFIC_NAMESPACE/g\" |\nkubectl apply -f -\n</code></pre></p> </li> </ol>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#verify-the-resources","title":"Verify the resources","text":"<pre><code>kubectl get pod -n nginx-sample-traffic \n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#visualization","title":"Visualization","text":"<ol> <li>Prometheus datasource on Grafana</li> <li> <p>After a successful deployment, this will open the Prometheus datasource configuration on Grafana. You should see a notification confirming that the Amazon Managed Service for Prometheus workspace is ready to be used on Grafana.</p> </li> <li> <p>Grafana dashboards</p> </li> <li>Go to the Dashboards panel of your Grafana workspace. You should see a list of dashboards under the <code>Observability Accelerator Dashboards</code>.</li> </ol> <p> </p> <ol> <li>Amazon Managed Service for Prometheus rules and alerts</li> <li>Open the Amazon Managed Service for Prometheus console and view the details of your workspace. Under the Rules management tab, you should find new rules deployed.</li> </ol> <p>To setup your alert receiver, with Amazon SNS, follow this documentation</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#verify-the-resources_1","title":"Verify the resources","text":"<p>Please see Single New Nginx Observability Accelerator.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-nginx-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-opensource-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-opensource-observability/","title":"Single Cluster Open Source Observability","text":""},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-opensource-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Existing EKS Cluster Open Source Observability pattern, using AWS native tools such as CloudWatch, Amazon Managed Service for Prometheus, Amazon Managed Grafana, and X-Ray and Open Source tools such as AWS Distro for OpenTelemetry (ADOT) and Prometheus Node Exporter.</p> <p></p> <p>Monitoring Amazon Elastic Kubernetes Service (Amazon EKS) for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-opensource-observability/#objective","title":"Objective","text":"<p>Configure the existing Amazon EKS cluster with below Observability components; - AWS Distro For OpenTelemetry Operator and Collector for Metrics and Traces - Logs with AWS for FluentBit - Installs Grafana Operator to add AWS data sources and create Grafana Dashboards to Amazon Managed Grafana. - Installs FluxCD to perform GitOps sync of a Git Repo to EKS Cluster. We will use this later for creating Grafana Dashboards and AWS datasources to Amazon Managed Grafana. You can also use your own GitRepo  to sync your own Grafana resources such as Dashboards, Datasources etc. Please check our One observability module - GitOps with Amazon Managed Grafana to learn more about this. - Installs External Secrets Operator to retrieve and Sync the Grafana API keys. - Amazon Managed Grafana Dashboard and data source - Alerts and recording rules with Amazon Managed Service for Prometheus</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-opensource-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol> <p>You will also need:</p> <ol> <li>Either an existing EKS cluster, or you can setup a new one with  Single New EKS Cluster Observability Accelerator</li> <li>An OpenID Connect (OIDC) provider, associated to the above EKS cluster (Note: Single EKS Cluster Pattern takes care of that for you)</li> </ol>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-opensource-observability/#deploying","title":"Deploying","text":"<p>!!! note If control plane logging is not enabled in the existing cluster, edit  <code>lib/existing-eks-opensource-observability-pattern/index.ts</code> to include <code>.enableControlPlaneLogging()</code> as shown below: <pre><code>    ObservabilityBuilder.builder()\n        // some properties\n        .enableControlPlaneLogging()\n        // other properties\n        .build(scope, stackId);\n</code></pre></p> <ol> <li>Edit <code>~/.cdk.json</code> by setting the name of your existing cluster:</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.cluster.name\": \"...\",\n        ...\n    }\n</code></pre> <ol> <li>Edit <code>~/.cdk.json</code> by setting the kubectl role name; if you used Single New EKS Cluster Observability Accelerator to setup your cluster, the kubectl role name would be provided by the output of the deployment, on your command-line interface (CLI):</li> </ol> <pre><code>    \"context\": {\n        ...\n        \"existing.kubectl.rolename\":\"...\",\n        ...\n    }\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>Warning</p> <p>Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens.</li> </ol> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository.</p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        }\n      ]\n    },\n  }\n</code></pre> <p>If you need Java observability you can instead use:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true\n  }\n</code></pre> <p>If you want to deploy API Server dashboards along with Java observability you can instead use:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\",\n        \"GRAFANA_APISERVER_BASIC_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-basic.json\",\n        \"GRAFANA_APISERVER_ADVANCED_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-advanced.json\",\n        \"GRAFANA_APISERVER_TROUBLESHOOTING_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-troubleshooting.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/apiserver\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true,\n    \"apiserver.pattern.enabled\": true\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern existing-eks-opensource-observability deploy\n</code></pre>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Please see Single New EKS Open Source Observability Accelerator.</p>"},{"location":"patterns/existing-eks-observability-accelerators/existing-eks-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern existing-eks-opensource-observability destroy\n</code></pre> <p>If you setup your cluster with Single New EKS Cluster Observability Accelerator, you also need to run:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/","title":"Multi-Cluster Multi-Account Multi-Region (M3) Observability","text":""},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Multi-Account Multi-Region Mixed Observability (M3) Accelerator using both AWS native tooling such as: CloudWatch ContainerInsights, CloudWatch logs and Open source tooling such as AWS Distro for Open Telemetry (ADOT), Amazon Managed Service for Prometheus (AMP), Amazon Managed Grafana :</p> <p></p>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#objective","title":"Objective","text":"<ol> <li>Deploying two production grade Amazon EKS cluster with control plane logging across two AWS Accounts (Prod1, Prod2 account) in two different regions through a Continuous Deployment infrastructure pipeline triggered upon a commit to the repository that holds the pipeline configuration in another AWS account (pipeline account).</li> <li>Deploying ADOT add-on, AMP add-on to Prod 1 Amazon EKS Cluster to remote-write metrics to AMP workspace in Prod 1 AWS Account.</li> <li>Deploying ADOT add-on, CloudWatch add-on to Prod 2 Amazon EKS Cluster to write metrics to CloudWatch in Prod 2 AWS Account.</li> <li>Configuring GitOps tooling (Argo CD add-on) to support deployment of ho11y and yelb sample applications, in a way that restricts each application to be deployed only into the team namespace, by using Argo CD projects.</li> <li>Setting up IAM roles in Prod 1 and Prod 2 Accounts to allow an AMG service role in the Monitoring account (mon-account) to access metrics from AMP workspace in Prod 1 account and CloudWatch namespace in Prod 2 account.</li> <li>Setting Amazon Managed Grafana to visualize AMP metrics from Amazon EKS cluster in Prod account 1 and CloudWatch metrics on workloads in Amazon EKS cluster in Prod account 2.</li> <li>Installing Grafana Operator in Monitoring account (mon-account) to add AWS data sources and create Grafana Dashboards to Amazon Managed Grafana.</li> <li>Installing External Secrets Operator in Monitoring account (mon-account) to retrieve and Sync the Grafana API keys.</li> </ol>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#gitops-configuration","title":"GitOps configuration","text":"<ul> <li>For GitOps, this pattern bootstraps Argo CD add-on and points to sample applications in AWS Observability Accelerator.</li> <li>You can find the team-geordie configuration for this pattern in the workload repository under the folder <code>team-geordie</code>.</li> <li>GitOps based management of Amazon Grafana resources (like: Datasources and Dashboards) is achieved using Argo CD application <code>grafana-operator-app</code>. Grafana Operator resources are deployed using <code>grafana-operator-chart</code>.</li> </ul>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure following tools are installed in host machine:</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> <li>argocd</li> <li>jq</li> </ol>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#aws-accounts","title":"AWS Accounts","text":"<ol> <li>AWS Control Tower deployed in your AWS environment in the management account. If you have not already installed AWS Control Tower, follow the Getting Started with AWS Control Tower documentation, or you can enable AWS Organizations in the AWS Management Console account and enable AWS SSO.</li> <li>An AWS account under AWS Control Tower called Prod 1 Account(Workloads Account A aka <code>prodEnv1</code>) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</li> <li>An AWS account under AWS Control Tower called Prod 2 Account(Workloads Account B aka <code>prodEnv2</code>) provisioned using the AWS Service Catalog Account Factory] product AWS Control Tower Account vending process or AWS Organization.</li> <li>An AWS account under AWS Control Tower called Pipeline Account (aka <code>pipelineEnv</code>) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</li> <li>An AWS account under AWS Control Tower called Monitoring Account (Grafana Account aka <code>monitoringEnv</code>) provisioned using the AWS Service Catalog Account Factory product AWS Control Tower Account vending process or AWS Organization.</li> <li>An existing Amazon Managed Grafana Workspace in <code>monitoringEnv</code> region of <code>monitoringEnv</code> account. Enable Data sources AWS X-Ray, Amazon Managed Service for Prometheus and Amazon Cloudwatch.</li> <li>If you are bringing new AWS accounts to deploy this pattern, then create a free-tier EC2 instance and let it run for 15-30 minutes in order to complete validation of account.</li> </ol> <p>NOTE: This pattern consumes multiple Elastic IP addresses, because 3 VPCs with 3 subnets are created in <code>ProdEnv1</code>, <code>ProdEnv2</code> and <code>monitoringEnv</code> AWS accounts. Make sure your account limits for EIP are increased to support additional 3 EIPs per account.</p>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#clone-repository","title":"Clone Repository","text":"<p>Clone <code>cdk-aws-observability-accelerator</code> repository, if not done already.</p> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\ncd cdk-aws-observability-accelerator\n</code></pre> <p>Pro Tip: This document is compatible to run as Notebook with RUNME for VS Code. There's no need to manually copy and paste commands. You can effortlessly execute them directly from this markdown file. Feel free to give it a try.</p> <p>Here is a sample usage of this document using RUNME:</p> <p></p>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#sso-profile-setup","title":"SSO Profile Setup","text":"<ol> <li>You will be accessing multiple accounts during deployment of this pattern. It is recommended to configure the AWS CLI to authenticate access with AWS IAM Identity Center (successor to AWS Single Sign-On). Let's configure Token provider with automatic authentication refresh for AWS IAM Identity Center. Ensure Prerequisites mentioned here are complete before proceeding to next steps.</li> <li>Create and use AWS IAM Identity Center login with <code>AWSAdministratorAccess</code> Permission set assigned to all AWS accounts required for this pattern (prodEnv1, prodEnv2, pipelineEnv and monitoringEnv).</li> <li>Configure AWS profile with sso for <code>pipelineEnv</code> account:</li> </ol> <pre><code>aws configure sso --profile pipeline-account\n</code></pre> <pre><code># sample configuration\n# SSO session name (Recommended): coa-multi-access-sso\n# SSO start URL [None]: https://d-XXXXXXXXXX.awsapps.com/start\n# SSO region [None]: us-west-2\n# SSO registration scopes [sso:account:access]:sso:account:access\n\n# Attempting to automatically open the SSO authorization page in your default browser.\n# If the browser does not open or you wish to use a different device to authorize this request, open the following URL:\n\n# https://device.sso.us-west-2.amazonaws.com/\n\n# Then enter the code:\n\n# XXXX-XXXX\n# There are 7 AWS accounts available to you.\n# Using the account ID 111122223333\n# There are 2 roles available to you.\n# Using the role name \"AWSAdministratorAccess\"\n# CLI default client Region [None]: us-west-2\n# CLI default output format [None]: json\n\n# To use this profile, specify the profile name using --profile, as shown:\n\n# aws s3 ls --profile pipeline-account\n</code></pre> <ol> <li>Then, configure profile for <code>ProdEnv1</code> AWS account.</li> </ol> <pre><code>aws configure sso --profile prod1-account\n</code></pre> <pre><code># sample configuration\n# SSO session name (Recommended): coa-multi-access-sso\n# There are 7 AWS accounts available to you.\n# Using the account ID 444455556666\n# There are 2 roles available to you.\n# Using the role name \"AWSAdministratorAccess\"\n# CLI default client Region [None]: us-west-2\n# CLI default output format [None]: json\n\n# To use this profile, specify the profile name using --profile, as shown:\n\n# aws s3 ls --profile prod2-account\n</code></pre> <ol> <li>Then, configure profile for <code>ProdEnv2</code> AWS account.</li> </ol> <pre><code>aws configure sso --profile prod2-account\n</code></pre> <ol> <li>Then, configure profile for <code>monitoringEnv</code> AWS account.</li> </ol> <pre><code>aws configure sso --profile monitoring-account\n</code></pre> <ol> <li>Login to required SSO profile using <code>aws sso login --profile &lt;profile name&gt;</code>. Let's now log in to <code>pipelineEnv</code> account. When SSO login expires, you can use this command to re-login.</li> </ol> <pre><code>export AWS_PROFILE='pipeline-account'\naws sso login --profile $AWS_PROFILE\n</code></pre> <ol> <li>Export required environment variables for further use. If not available already, you will be prompted for name of Amazon Grafana workspace in <code>monitoringEnv</code> region of <code>monitoringEnv</code> account. And, then its endpoint URL, ID, Role ARN will be captured as environment variables.</li> </ol> <pre><code>source `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/source-envs.sh\n</code></pre> <ol> <li>Create SSM SecureString Parameter <code>/cdk-accelerator/cdk-context</code> in <code>pipelineEnv</code> region of <code>pipelineEnv</code> account. This parameter contains account ID and region of all four AWS accounts used in this Observability Accelerator pattern.</li> </ol> <pre><code>aws ssm put-parameter --profile pipeline-account --region ${COA_PIPELINE_REGION} \\\n    --type \"SecureString\" \\\n    --overwrite \\\n    --name \"/cdk-accelerator/cdk-context\" \\\n    --description \"AWS account details of different environments used by Multi-Account Multi-Region Mixed Observability (M3) Accelerator pattern\" \\\n    --value '{\n        \"context\": {\n            \"pipelineEnv\": {\n                \"account\": \"'$COA_PIPELINE_ACCOUNT_ID'\",\n                \"region\": \"'$COA_PIPELINE_REGION'\"\n\n            },\n            \"prodEnv1\": {\n                \"account\": \"'$COA_PROD1_ACCOUNT_ID'\",\n                \"region\": \"'$COA_PROD1_REGION'\"\n            },\n            \"prodEnv2\": {\n                \"account\": \"'$COA_PROD2_ACCOUNT_ID'\",\n                \"region\": \"'$COA_PROD2_REGION'\"\n            },\n            \"monitoringEnv\": {\n                \"account\": \"'$COA_MON_ACCOUNT_ID'\",\n                \"region\": \"'$COA_MON_REGION'\"\n            }\n        }\n    }'\n</code></pre>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#amazon-grafana-configuration","title":"Amazon Grafana Configuration","text":"<ol> <li> <p>Run <code>helpers/multi-acc-new-eks-mixed-observability-pattern/amg-preconfig.sh</code> script to</p> </li> <li> <p>create SSM SecureString parameter <code>/cdk-accelerator/amg-info</code> in <code>pipelineEnv</code> region of <code>pipelineEnv</code> account. This will be used by CDK for Grafana Operator resources configuration.</p> </li> <li>create Grafana workspace API key.</li> <li>create SSM SecureString parameter <code>/cdk-accelerator/grafana-api-key</code> in <code>monitoringEnv</code> region of <code>monitoringEnv</code> account. This will be used by External Secrets Operator.</li> </ol> <pre><code>eval bash `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/amg-preconfig.sh\n</code></pre>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#github-sources-configuration","title":"GitHub Sources Configuration","text":"<ol> <li> <p>Following GitHub sources used in this pattern:</p> </li> <li> <p>Apps Git Repo - This repository serves as the source for deploying and managing applications in <code>prodEnv1</code> and <code>prodEnv2</code> using GitOps by Argo CD. Here, it is configured to sample-apps from aws-observability-accelerator.</p> </li> <li>Source for CodePipeline - This repository serves as the CodePipeline source stage for retrieving and providing source code to downstream pipeline stages, facilitating automated CI/CD processes. Whenever a change is detected in the source code, the pipeline is initiated automatically. This is achieved using GitHub webhooks. We are using CodePipeline to deploy multi-account multi-region clusters.</li> <li>Source for <code>monitoringEnv</code> Argo CD - This repository serves as the source for deploying and managing applications in the <code>monitoringEnv</code> environment using GitOps by Argo CD. Here, it is configured to grafana-operator-app from aws-observability-accelerator, using which Grafana Datasoures and Dashboards are deployed.</li> </ol> <p>NOTE: Argo CD source repositories used here for <code>prodEnv1</code>, <code>prodEnv2</code> and <code>monitoringEnv</code> are public. If you need to use private repositories, create secret called <code>github-ssh-key</code> in respective accounts and region. This secret should contain your GitHub SSH private key as a JSON structure with fields <code>sshPrivateKey</code> and <code>url</code> in AWS Secrets Manager. Argo CD add-on will use this secret for authentication with private GitHub repositories. For more details on setting up SSH credentials, please refer to Argo CD Secrets Support.</p> <ol> <li>Fork <code>cdk-aws-observability-accelerator</code> repository to your GitHub account.</li> <li> <p>Create GitHub Personal Access Token (PAT) for your CodePipeline GitHub source. For more information on how to set it up, please refer here. The GitHub Personal Access Token should have these scopes:</p> </li> <li> <p>repo - to read the repository</p> </li> <li> <p>admin:repo_hook - to use webhooks</p> </li> <li> <p>Run <code>helpers/multi-acc-new-eks-mixed-observability-pattern/gitsource-preconfig.sh</code> script to</p> </li> <li> <p>create SSM SecureString Parameter <code>/cdk-accelerator/pipeline-git-info</code> in <code>pipelineEnv</code> region of <code>pipelineEnv</code> account which contains details of CodePipeline source. This parameter contains GitHub owner name where you forked <code>cdk-aws-observability-accelerator</code>, repository name (<code>cdk-aws-observability-accelerator</code>) and branch (<code>main</code>).</p> </li> <li>create AWS Secret Manager secret <code>github-token</code> in <code>pipelineEnv</code> region of <code>pipelineEnv</code> account to hold GitHub Personal Access Token (PAT).</li> </ol> <pre><code>eval bash `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/gitsource-preconfig.sh\n</code></pre>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#deployment","title":"Deployment","text":"<ol> <li>Fork <code>cdk-aws-observability-accelerator</code> repository to your CodePioeline source GitHub organization/user.</li> <li>Install the AWS CDK Toolkit globally on host machine.</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li>Install project dependencies.</li> </ol> <pre><code>cd `git rev-parse --show-toplevel`\nnpm i\nmake build\n</code></pre> <ol> <li>Bootstrap all 4 AWS accounts using step mentioned for different environment for deploying CDK applications in Deploying Pipelines. If you have bootstrapped earlier, please remove them before proceeding with this step. Remember to set <code>pipelineEnv</code> account number in <code>--trust</code> flag. You can also refer to commands mentioned below:</li> </ol> <pre><code># bootstrap pipelineEnv account WITHOUT explicit trust\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap --profile pipeline-account \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    aws://${COA_PIPELINE_ACCOUNT_ID}/${COA_PIPELINE_REGION}\n\n# bootstrap prodEnv1 account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap --profile prod1-account \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    --trust ${COA_PIPELINE_ACCOUNT_ID} \\\n    aws://${COA_PROD1_ACCOUNT_ID}/${COA_PROD1_REGION}\n\n# bootstrap prodEnv2 account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap --profile prod2-account \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    --trust ${COA_PIPELINE_ACCOUNT_ID} \\\n    aws://${COA_PROD2_ACCOUNT_ID}/${COA_PROD2_REGION}\n\n# bootstrap monitoringEnv account with trust access from pipelineEnv account\nenv CDK_NEW_BOOTSTRAP=1 npx cdk bootstrap --profile monitoring-account \\\n    --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\n    --trust ${COA_PIPELINE_ACCOUNT_ID} \\\n    aws://${COA_MON_ACCOUNT_ID}/${COA_MON_REGION}\n</code></pre> <ol> <li>Once all pre-requisites are set, you are ready to deploy the pipeline. Run the following command from the root of cloned repository to deploy the pipeline stack in <code>pipelineEnv</code> account. This step may require approximately 20 minutes to finish.</li> </ol> <pre><code>export AWS_PROFILE='pipeline-account'\nexport AWS_REGION=${COA_PIPELINE_REGION}\ncd `git rev-parse --show-toplevel`\n\nmake pattern multi-acc-new-eks-mixed-observability deploy multi-account-COA-pipeline\n</code></pre> <ol> <li> <p>Check status of pipeline that deploys multiple Amazon EKS clusters through CloudFromation stacks in respective accounts. This deployment also creates</p> </li> <li> <p><code>ampPrometheusDataSourceRole</code> with permissions to retrieve metrics from AMP in <code>ProdEnv1</code> account,</p> </li> <li><code>cloudwatchDataSourceRole</code> with permissions to retrieve metrics from CloudWatch in <code>ProdEnv2</code> account and</li> <li>Updates Amazon Grafana workspace IAM role in <code>monitoringEnv</code> account to assume roles in <code>ProdEnv1</code> and <code>ProdEnv2</code> accounts for retrieving and visualizing metrics in Grafana</li> </ol> <p>This step may require approximately 50 minutes to finish. You may login to <code>pipelineEnv</code> account and navigate to AWS CodePipeline console at <code>pipelineEnv</code> region to check the status.</p> <pre><code># script to check status of codepipeline\ndots=\"\"; while true; do status=$(aws codepipeline --profile pipeline-account list-pipeline-executions --pipeline-name multi-account-COA-pipeline --query 'pipelineExecutionSummaries[0].status' --output text); [ $status == \"Succeeded\" ] &amp;&amp; echo -e \"Pipeline execution SUCCEEDED.\" &amp;&amp; break || [ \"$status\" == \"Failed\" ] &amp;&amp; echo -e \"Pipeline execution FAILED.\" &amp;&amp; break ||  printf \"\\r\" &amp;&amp; echo -n \"Pipeline execution status: $status$dots\" &amp;&amp; dots+=\".\" &amp;&amp; sleep 10; done\n</code></pre>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#post-deployment","title":"Post Deployment","text":"<ol> <li> <p>Once all steps of <code>multi-acc-stages</code> in <code>multi-account-COA-pipeline</code> are complete, run script to</p> </li> <li> <p>create entries in kubeconfig with contexts of newly created EKS clusters.</p> </li> <li>export cluster specific and kubecontext environment variables (like: <code>COA_PROD1_CLUSTER_NAME</code> and <code>COA_PROD1_KUBE_CONTEXT</code>).</li> <li>get Amazon Prometheus Endpoint URL from <code>ProdEnv1</code> account and export to environment variable <code>COA_AMP_ENDPOINT_URL</code>.</li> </ol> <pre><code>source `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/post-deployment-source-envs.sh\n</code></pre> <ol> <li>Then, update parameter <code>AMP_ENDPOINT_URL</code> of Argo CD bootstrap app in <code>monitoringEnv</code> with Amazon Prometheus endpoint URL from <code>ProdEnv1</code> account (<code>COA_AMP_ENDPOINT_URL</code>) and sync Argo CD apps.</li> </ol> <pre><code>if [[ `lsof -i:8080 | wc -l` -eq 0 ]]\nthen\n    export ARGO_SERVER=$(kubectl --context ${COA_MON_KUBE_CONTEXT} -n argocd get svc -l app.kubernetes.io/name=argocd-server -o name)\n    export ARGO_PASSWORD=$(kubectl --context ${COA_MON_KUBE_CONTEXT} -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d)\n    echo \"ARGO PASSWORD:: \"$ARGO_PASSWORD\n    kubectl --context ${COA_MON_KUBE_CONTEXT} port-forward $ARGO_SERVER -n argocd 8080:443 &gt; /dev/null 2&gt;&amp;1 &amp;\n    argocdPid=$!\n    echo pid: $argocdPid\n    sleep 5s\n\n    argocd --kube-context ${COA_MON_KUBE_CONTEXT} login localhost:8080 --insecure --username admin --password $ARGO_PASSWORD\n\n    argocd --kube-context ${COA_MON_KUBE_CONTEXT} app set argocd/bootstrap-apps --helm-set AMP_ENDPOINT_URL=$COA_AMP_ENDPOINT_URL\n    argocd --kube-context ${COA_MON_KUBE_CONTEXT} app sync argocd/bootstrap-apps\n\n    echo -e '\\033[0;33m' \"\\nConfirm update here.. You should see AMP endpoint URL and no error message.\" '\\033[0m'\n    kubectl --context ${COA_MON_KUBE_CONTEXT} get -n grafana-operator grafanadatasources grafanadatasource-amp -o jsonpath='{.spec.datasource.url}{\"\\n\"}{.status}{\"\\n\"}'\n\n    kill -9 $argocdPid\nelse\n    echo \"Port 8080 is already in use by PID `lsof -i:8080 -t`. Please terminate it and rerun this step.\"\nfi\n</code></pre> <p>NOTE: You can access Argo CD Admin UI using port-forwading. Here are commands to access <code>prodEnv1</code> Argo CD:</p> <pre><code>export PROD1_ARGO_SERVER=$(kubectl --context ${COA_PROD1_KUBE_CONTEXT} -n argocd get svc -l app.kubernetes.io/name=argocd-server -o name)\nexport PROD1_ARGO_PASSWORD=$(kubectl --context ${COA_PROD1_KUBE_CONTEXT} -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d)\necho \"PROD1 ARGO PASSWORD:: \"$PROD1_ARGO_PASSWORD\nnohup kubectl --context ${COA_PROD1_KUBE_CONTEXT} port-forward $PROD1_ARGO_SERVER -n argocd 8081:443 &gt; /dev/null 2&gt;&amp;1 &amp;\nsleep 5\ncurl localhost:8081\n</code></pre> <ol> <li>Datasource <code>grafana-operator-amp-datasource</code> created by Grafana Operator needs to reflect AMP Endpoint URL. There is a limitation with Grafana Operator (or Grafana) which doesn't sync updated <code>grafana-datasources</code> to Grafana. To overcome this issue, we will simply delete Datasource and Grafana Operator syncs up with the latest configuration in 5 minutes. This is achieved using Grafana API and key stored in SecureString parameter <code>/cdk-accelerator/grafana-api-key</code> in <code>monitoringEnv</code> account.</li> </ol> <pre><code>export COA_AMG_WORKSPACE_URL=$(aws ssm get-parameter --profile pipeline-account --region ${COA_PIPELINE_REGION} \\\n    --name \"/cdk-accelerator/amg-info\" \\\n    --with-decryption \\\n    --query Parameter.Value --output text | jq -r \".[] | .workspaceURL\")\n\nexport COA_AMG_API_KEY=$(aws ssm get-parameter --profile monitoring-account --region ${COA_MON_REGION} \\\n    --name \"/cdk-accelerator/grafana-api-key\" \\\n    --with-decryption \\\n    --query Parameter.Value --output text)\n\nexport COA_AMP_DS_ID=$(curl -s -H \"Authorization: Bearer ${COA_AMG_API_KEY}\" ${COA_AMG_WORKSPACE_URL}/api/datasources \\\n    | jq -r \".[] |  select(.name==\\\"grafana-operator-amp-datasource\\\") | .id\")\n\necho \"Datasource Name:: grafana-operator-amp-datasource\"\necho \"Datasource ID:: \"$COA_AMP_DS_ID\n\ncurl -X DELETE -H \"Authorization: Bearer ${COA_AMG_API_KEY}\" ${COA_AMG_WORKSPACE_URL}/api/datasources/${COA_AMP_DS_ID}\n</code></pre> <ol> <li>Then, deploy ContainerInsights in <code>ProdEnv2</code> account.</li> </ol> <pre><code>prod2NGRole=$(aws cloudformation describe-stack-resources --profile prod2-account --region ${COA_PROD2_REGION} \\\n    --stack-name \"coa-eks-prod2-${COA_PROD2_REGION}-coa-eks-prod2-${COA_PROD2_REGION}-blueprint\" \\\n    --query \"StackResources[?ResourceType=='AWS::IAM::Role' &amp;&amp; contains(LogicalResourceId,'NodeGroupRole')].PhysicalResourceId\" \\\n    --output text)\n\naws iam attach-role-policy --profile prod2-account --region ${COA_PROD2_REGION} \\\n    --role-name ${prod2NGRole} \\\n    --policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy\n\naws iam list-attached-role-policies --profile prod2-account --region ${COA_PROD2_REGION} \\\n    --role-name $prod2NGRole | grep CloudWatchAgentServerPolicy || echo 'Policy not found'\n\nFluentBitHttpPort='2020'\nFluentBitReadFromHead='Off'\n[[ ${FluentBitReadFromHead} = 'On' ]] &amp;&amp; FluentBitReadFromTail='Off'|| FluentBitReadFromTail='On'\n[[ -z ${FluentBitHttpPort} ]] &amp;&amp; FluentBitHttpServer='Off' || FluentBitHttpServer='On'\ncurl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluent-bit-quickstart.yaml | sed 's/{{cluster_name}}/'${COA_PROD2_CLUSTER_NAME}'/;s/{{region_name}}/'${COA_PROD2_REGION}'/;s/{{http_server_toggle}}/\"'${FluentBitHttpServer}'\"/;s/{{http_server_port}}/\"'${FluentBitHttpPort}'\"/;s/{{read_from_head}}/\"'${FluentBitReadFromHead}'\"/;s/{{read_from_tail}}/\"'${FluentBitReadFromTail}'\"/' | kubectl --context ${COA_PROD2_KUBE_CONTEXT} apply -f -\n</code></pre>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#validating-grafana-dashboards","title":"Validating Grafana Dashboards","text":"<ol> <li>Run the below command in <code>ProdEnv1</code> cluster to generate test traffic to sample application and let us visualize traces to X-Ray and Amazon Managed Grafana Console out the sample <code>ho11y</code> app :</li> </ol> <pre><code>frontend_pod=`kubectl --context ${COA_PROD1_KUBE_CONTEXT} get pod -n geordie --no-headers -l app=frontend -o jsonpath='{.items[*].metadata.name}'`\nloop_counter=0\nwhile [ $loop_counter -le 5000 ] ;\ndo\n        kubectl exec --context ${COA_PROD1_KUBE_CONTEXT} -n geordie -it $frontend_pod -- curl downstream0.geordie.svc.cluster.local;\n        echo ;\n        loop_counter=$[$loop_counter+1];\ndone\n</code></pre> <ol> <li>Let it run for a few minutes and look in Amazon Grafana Dashboards &gt; Observability Accelerator Dashboards &gt; Kubernetes / Compute Resources / Namespace (Workloads)</li> </ol> <p>Please also have a look at other Dashboards created using Grafana Operator under folder Observability Accelerator Dashboards.</p> <ol> <li>Run the below command in <code>ProdEnv2</code> cluster to generate test traffic to sample application.</li> </ol> <pre><code>frontend_pod=`kubectl --context ${COA_PROD2_KUBE_CONTEXT} get pod -n geordie --no-headers -l app=frontend -o jsonpath='{.items[*].metadata.name}'`\nloop_counter=0\nwhile [ $loop_counter -le 5000 ] ;\ndo\n        kubectl exec --context ${COA_PROD2_KUBE_CONTEXT} -n geordie -it $frontend_pod -- curl downstream0.geordie.svc.cluster.local;\n        echo ;\n        loop_counter=$[$loop_counter+1];\ndone\n</code></pre> <ol> <li>Let it run for a few minutes and look in Amazon Grafana Administration &gt; Datasources &gt; grafana-operator-cloudwatch-datasource &gt; Explore. Set values as highlighted in the snapshot and 'Run query'.</li> </ol> <p></p> <ol> <li>Then, let us look at X-Ray traces in Amazon Grafana Administration &gt; Datasources &gt; grafana-operator-xray-datasource &gt; Explore. Set Query Type = Service Map and 'Run query'.</li> </ol> <p></p>"},{"location":"patterns/multi-new-eks-observability-accelerators/multi-acc-new-eks-mixed-observability/#clean-up","title":"Clean up","text":"<ol> <li>Run this command to destroy this pattern. This will delete pipeline.</li> </ol> <pre><code>export AWS_PROFILE='pipeline-account'\naws sso login --profile $AWS_PROFILE\ncd `git rev-parse --show-toplevel`\n\nsource `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/source-envs.sh\nmake pattern multi-acc-new-eks-mixed-observability destroy multi-account-COA-pipeline\n</code></pre> <ol> <li>Next, run this script to clean up resources created in respective accounts. This script deletes Argo CD apps, unsets kubeconfig entries, initiates deletion of CloudFormation stacks, secrets, SSM parameters and Amazon Grafana Workspace API key from respective accounts.</li> </ol> <pre><code>eval bash `git rev-parse --show-toplevel`/helpers/multi-acc-new-eks-mixed-observability-pattern/clean-up.sh\n</code></pre> <ol> <li>In certain scenarios, CloudFormation stack deletion might encounter issues when attempting to delete a nodegroup IAM role. In such situations, it's recommended to first delete the relevant IAM role and then proceed with deleting the CloudFormation stack.</li> <li>Delete Dashboards and Data sources in Amazon Grafana.</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-adotmetrics-collection-opensource-observability/","title":"Single Cluster Open Source Observability - OTEL Collector Monitoring","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-adotmetrics-collection-opensource-observability/#objective","title":"Objective","text":"<p>This pattern demonstrates how to use the New EKS Cluster Open Source Observability Accelerator with monitoring for ADOT collector health.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-adotmetrics-collection-opensource-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-adotmetrics-collection-opensource-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern, except for step 7, where you need to replace \"context\" in <code>~/.cdk.json</code> with the following:</p> <pre><code> \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_ADOTHEALTH_DASH_URL\": \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/adot/adothealth.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/adot\"\n        }\n      ]\n    },\n    \"adotcollectormetrics.pattern.enabled\": true\n  }\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-adotmetrics-collection-opensource-observability/#visualization","title":"Visualization","text":"<p>The OpenTelemetry collector produces metrics to monitor the entire pipeline.</p> <p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see three new dashboard named <code>OpenTelemetry Health Collector</code>, under <code>Observability Accelerator Dashboards</code></p> <p>This dashboard shows useful telemetry information about the ADOT collector itself which can be helpful when you want to troubleshoot any issues with the collector or understand how much resources the collector is consuming.</p> <p>Below diagram shows an example data flow and the components in an ADOT collector:</p> <p></p> <p>In this dashboard, there are five sections. Each section has metrics relevant to the various components of the AWS Distro for OpenTelemetry (ADOT) collector :</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-adotmetrics-collection-opensource-observability/#receivers","title":"Receivers","text":"<p>Shows the receiver\u2019s accepted and refused rate/count of spans and metric points that are pushed into the telemetry pipeline.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-adotmetrics-collection-opensource-observability/#processors","title":"Processors","text":"<p>Shows the accepted and refused rate/count of spans and metric points pushed into next component in the pipeline. The batch metrics can help to understand how often metrics are sent to exporter and the batch size.</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-adotmetrics-collection-opensource-observability/#exporters","title":"Exporters","text":"<p>Shows the exporter\u2019s accepted and refused rate/count of spans and metric points that are pushed to any of the destinations. It also shows the size and capacity of the retry queue. These metrics can be used to understand if the collector is having issues in sending trace or metric data to the destination configured.</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-adotmetrics-collection-opensource-observability/#collectors","title":"Collectors","text":"<p>Shows the collector\u2019s operational metrics (Memory, CPU, uptime). This can be used to understand how much resources the collector is consuming.</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-adotmetrics-collection-opensource-observability/#data-flow","title":"Data Flow","text":"<p>Shows the metrics and spans data flow through the collector\u2019s components.</p> <p></p> <p>Note:     To read more about the metrics and the dashboard used, visit the upstream documentation here.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-adotmetrics-collection-opensource-observability/#disable-adot-health-monitoring","title":"Disable ADOT health monitoring","text":"<p>Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>   \"context\": {\n    \"adotcollectormetrics.pattern.enabled\": false\n  }\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-adotmetrics-collection-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-apiserver-opensource-observability/","title":"Single Cluster Open Source Observability - API Server Monitoring","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-apiserver-opensource-observability/#objective","title":"Objective","text":"<p>This pattern demonstrates how to use the New EKS Cluster Open Source Observability Accelerator with API Server monitoring.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-apiserver-opensource-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-apiserver-opensource-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern, except for step 7, where you need to replace \"context\" in <code>~/.cdk.json</code> with the following:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_APISERVER_BASIC_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-basic.json\",\n        \"GRAFANA_APISERVER_ADVANCED_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-advanced.json\",\n        \"GRAFANA_APISERVER_TROUBLESHOOTING_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-troubleshooting.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/apiserver\"\n        }\n      ]\n    },\n    \"apiserver.pattern.enabled\": true,\n  }\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-apiserver-opensource-observability/#visualization","title":"Visualization","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see three new dashboard named <code>Kubernetes/Kube-apiserver (basic), Kubernetes/Kube-apiserver (advanced), Kubernetes/Kube-apiserver (troubleshooting)</code>, under <code>Observability Accelerator Dashboards</code>:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (basic)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (advanced)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p> <p>Open the <code>Kubernetes/Kube-apiserver (troubleshooting)</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-apiserver-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/","title":"Single Cluster AWS Native Observability - Fargate","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Fargate Cluster Native Observability pattern using AWS native tools such as CloudWatch Logs and Container Insights.</p> <p></p> <p>This example makes use of CloudWatch Container Insights as a vizualization and metric-aggregation layer. Amazon CloudWatch Container Insights helps customers collect, aggregate, and summarize metrics and logs from containerized applications and microservices. Metrics data is collected as performance log events using the embedded metric format. These performance log events use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards.</p> <p>AWS Distro for OpenTelemetry (ADOT) is a secure, AWS-supported distribution of the OpenTelemetry project. With ADOT, users can instrument their applications just once to send correlated metrics and traces to multiple monitoring solutions. With ADOT support for CloudWatch Container Insights, customers can collect system metrics such as CPU, memory, disk, and network usage from Amazon EKS clusters running on Amazon Elastic Cloud Compute (Amazon EC2), providing the same experience as Amazon CloudWatch agent. In EKS Fargate networking architecture, a pod is not allowed to directly reach the kubelet on that worker node. Hence, the ADOT Collector calls the Kubernetes API Server to proxy the connection to the kubelet on a worker node, and collect kubelet\u2019s cAdvisor metrics for workloads on that node. </p> <p>By combining Container Insights and CloudWatch logs, we are able to provide a foundation for EKS (Amazon Elastic Kubernetes Service) Observability. Monitoring EKS for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS Fargate cluster</li> <li>Enables Control Plane Logging. </li> <li>Logs with CloudWatch Logs</li> <li>Enables CloudWatch Container Insights.</li> <li>Installs Prometheus Node Exporter and Metrics Server for infrastructure metrics.</li> </ul>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</p> </li> </ol> <pre><code>make build\nmake pattern single-new-eks-awsnative-fargate-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-awsnative-fargate-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-awsnative-singleneweksawsnativeobs-xxxxxxxx\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> <pre><code>Output:\nNAME                                   STATUS   ROLES    AGE   VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nfargate-ip-10-0-102-84.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.102.84    &lt;none&gt;        Amazon Linux 2   5.10.184-175.749.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-124-175.ec2.internal   Ready    &lt;none&gt;   12m   v1.27.1-eks-2f008fe   10.0.124.175   &lt;none&gt;        Amazon Linux 2   5.10.184-175.749.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-126-244.ec2.internal   Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.126.244   &lt;none&gt;        Amazon Linux 2   5.10.184-175.749.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-132-165.ec2.internal   Ready    &lt;none&gt;   12m   v1.27.1-eks-2f008fe   10.0.132.165   &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-159-96.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.159.96    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-170-28.ec2.internal    Ready    &lt;none&gt;   14m   v1.27.1-eks-2f008fe   10.0.170.28    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-173-57.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.173.57    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-175-87.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.175.87    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-187-27.ec2.internal    Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.187.27    &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-188-225.ec2.internal   Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.188.225   &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-189-234.ec2.internal   Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.189.234   &lt;none&gt;        Amazon Linux 2   5.10.186-179.751.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-96-29.ec2.internal     Ready    &lt;none&gt;   15m   v1.27.1-eks-2f008fe   10.0.96.29     &lt;none&gt;        Amazon Linux 2   5.10.184-175.749.amzn2.x86_64   containerd://1.6.6\n</code></pre></p> <p><pre><code>kubectl get pods -o wide -A\n</code></pre> <pre><code>NAMESPACE                       NAME                                                   READY   STATUS    RESTARTS       AGE   IP             NODE                                   NOMINATED NODE   READINESS GATES\ncert-manager                    cert-manager-875c7579b-5kzg5                           1/1     Running   0              17m   10.0.188.225   fargate-ip-10-0-188-225.ec2.internal   &lt;none&gt;           &lt;none&gt;\ncert-manager                    cert-manager-cainjector-7bb6786867-xrtbx               1/1     Running   0              17m   10.0.102.84    fargate-ip-10-0-102-84.ec2.internal    &lt;none&gt;           &lt;none&gt;\ncert-manager                    cert-manager-webhook-79d574fbd5-9b7mx                  1/1     Running   0              17m   10.0.187.27    fargate-ip-10-0-187-27.ec2.internal    &lt;none&gt;           &lt;none&gt;\ndefault                         otel-collector-cloudwatch-collector-65bb5d7cb6-x8gdl   1/1     Running   1 (114s ago)   14m   10.0.132.165   fargate-ip-10-0-132-165.ec2.internal   &lt;none&gt;           &lt;none&gt;\ndefault                         otel-collector-xray-collector-796b57b657-tnx86         1/1     Running   0              14m   10.0.124.175   fargate-ip-10-0-124-175.ec2.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     aws-load-balancer-controller-8dcffbf6c-6qgfn           1/1     Running   0              17m   10.0.96.29     fargate-ip-10-0-96-29.ec2.internal     &lt;none&gt;           &lt;none&gt;\nkube-system                     aws-load-balancer-controller-8dcffbf6c-dgqn6           1/1     Running   0              17m   10.0.189.234   fargate-ip-10-0-189-234.ec2.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     blueprints-addon-metrics-server-6765c9bc59-v98h5       1/1     Running   0              17m   10.0.175.87    fargate-ip-10-0-175-87.ec2.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     coredns-788dbcccd5-7lf2g                               1/1     Running   0              17m   10.0.173.57    fargate-ip-10-0-173-57.ec2.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     coredns-788dbcccd5-wn8nc                               1/1     Running   0              17m   10.0.126.244   fargate-ip-10-0-126-244.ec2.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     kube-state-metrics-7f4b8b9f5-g994r                     1/1     Running   0              17m   10.0.159.96    fargate-ip-10-0-159-96.ec2.internal    &lt;none&gt;           &lt;none&gt;\nopentelemetry-operator-system   opentelemetry-operator-5fbdd4f5f9-lm2nf                2/2     Running   0              16m   10.0.170.28    fargate-ip-10-0-170-28.ec2.internal    &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p><pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <pre><code>NAME                       STATUS   AGE\naws-for-fluent-bit              Active   17m\ncert-manager                    Active   17m\ndefault                         Active   27m\nkube-node-lease                 Active   27m\nkube-public                     Active   27m\nkube-system                     Active   27m\nopentelemetry-operator-system   Active   17m\n</code></pre></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#viewing-logs","title":"Viewing Logs","text":"<p>By default, we deploy a FluentBit daemon set in the cluster to collect worker logs for all namespaces. Logs are collected and exported to Amazon CloudWatch Logs, which enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#using-cloudwatch-logs-insights-to-query-logs","title":"Using CloudWatch Logs Insights to Query Logs","text":"<p>Navigate to CloudWatch, then go to \"Logs Insights\"</p> <p>In the dropdown, select any of the logs that begin with \"/aws/eks/single-new-eks-awsnative-fargate-observability-accelerator\" and run a query.</p> <p>Example with \"kubesystem\" log group:</p> <p></p> <p>Then you can view the results of your query:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#viewing-metrics","title":"Viewing Metrics","text":"<p>Metrics are collected by the cloudWatchAdotAddon as based on the metricsNameSelectors we defined (default <code>['apiserver_request_.*', 'container_memory_.*', 'container_threads', 'otelcol_process_.*']</code>). These metrics can be found in the Cloudwatch metrics dashboard. </p> <p>Navigate to Cloudwatch, then go to \"Metrics\"</p> <p>Select \"All Metrics\" from the dropdown and select any logs in the ContainerInsights namespace</p> <p>Example with \"EKS_Cluster\" metrics</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#monitoring-workloads-on-eks","title":"Monitoring workloads on EKS","text":"<p>Although the default metrics exposed by cloudWatchAdotAddon are useful for getting some standardized metrics from our application we often instrument our own application with OLTP to expose metrics. Fortunately, the otel-collector-cloudwatch-collector can be specified as the endpoint for collecting these metrics and getting metrics and logs to cloudwatch. </p> <p>We will be fetching metrics from <code>ho11y</code> a synthetic signal generator allowing you to test observability solutions for microservices. It emits logs, metrics, and traces in a configurable manner. </p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#deploying-workload","title":"Deploying Workload","text":"<pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: ho11y\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: ho11y\n        image: public.ecr.aws/z0a4o2j5/ho11y:latest\n        ports:\n        - containerPort: 8765\n        env:\n        - name: DISABLE_OM\n          value: \"on\"\n        - name: HO11Y_LOG_DEST\n          value: \"stdout\"\n        - name: OTEL_RESOURCE_ATTRIB\n          value: \"frontend\"\n        - name: OTEL_EXPORTER_OTLP_ENDPOINT\n          value: \"otel-collector-cloudwatch-collector.default.svc.cluster.local:4317\"\n        - name: HO11Y_INJECT_FAILURE\n          value: \"enabled\"\n        - name: DOWNSTREAM0\n          value: \"http://downstream0\"\n        - name: DOWNSTREAM1\n          value: \"http://downstream1\"\n        imagePullPolicy: Always\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: downstream0\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: downstream0\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: downstream0\n    spec:\n      containers:\n      - name: ho11y\n        image: public.ecr.aws/mhausenblas/ho11y:stable\n        ports:\n        - containerPort: 8765\n        env:\n        - name: DISABLE_OM\n          value: \"on\"\n        - name: HO11Y_LOG_DEST\n          value: \"stdout\"\n        - name: OTEL_RESOURCE_ATTRIB\n          value: \"downstream0\"\n        - name: OTEL_EXPORTER_OTLP_ENDPOINT\n          value: \"otel-collector-cloudwatch-collector.default.svc.cluster.local:4317\"\n        - name: DOWNSTREAM0\n          value: \"https://mhausenblas.info/\"\n        imagePullPolicy: Always\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: downstream1\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: downstream1\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: downstream1\n    spec:\n      containers:\n      - name: ho11y\n        image: public.ecr.aws/mhausenblas/ho11y:stable\n        ports:\n        - containerPort: 8765\n        env:\n        - name: DISABLE_OM\n          value: \"on\"\n        - name: HO11Y_LOG_DEST\n          value: \"stdout\"\n        - name: OTEL_RESOURCE_ATTRIB\n          value: \"downstream1\"\n        - name: OTEL_EXPORTER_OTLP_ENDPOINT\n          value: \"otel-collector-cloudwatch-collector.default.svc.cluster.local:4317\"\n        - name: DOWNSTREAM0\n          value: \"https://o11y.news/2021-03-01/\"\n        - name: DOWNSTREAM1\n          value: \"DUMMY:187kB:42ms\"\n        - name: DOWNSTREAM2\n          value: \"DUMMY:13kB:2ms\"\n        imagePullPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\n  namespace: default\n  annotations:\n    scrape: \"true\"\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 8765\n  selector:\n    app: frontend\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: downstream0\n  namespace: default\n  annotations:\n    scrape: \"true\"\nspec:\n  ports:\n  - port: 80\n    targetPort: 8765\n  selector:\n    app: downstream0\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: downstream1\n  namespace: default\n  annotations:\n    scrape: \"true\"\nspec:\n  ports:\n  - port: 80\n    targetPort: 8765\n  selector:\n    app: downstream1\n---\nEOF\n</code></pre> <p>To verify the Pod was successfully deployed, please run:</p> <pre><code>kubectl get pods\n</code></pre> <pre><code>kubectl get pods\nNAME                                                   READY   STATUS    RESTARTS   AGE\ndownstream0-6b665bbfd-6zdsb                            1/1     Running   0          61s\ndownstream1-749d75f6c-9t5dl                            1/1     Running   0          61s\nfrontend-557fd48b4f-gx8ff                              1/1     Running   0          61s\n</code></pre> <p>Once deployed you will be able to monitor the Ho11y metrics in cloudwatch as shown:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-fargate-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-awsnative-fargate-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/","title":"Single Cluster AWS Native Observability","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Cluster Native Observability pattern using AWS native tools such as CloudWatch Logs and Container Insights.</p> <p></p> <p>This example makes use of CloudWatch Container Insights as a vizualization and metric-aggregation layer. Amazon CloudWatch Container Insights helps customers collect, aggregate, and summarize metrics and logs from containerized applications and microservices. Metrics data is collected as performance log events using the embedded metric format. These performance log events use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards.</p> <p>By combining Container Insights and CloudWatch logs, we are able to provide a foundation for EKS (Amazon Elastic Kubernetes Service) Observability. Monitoring EKS for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster.</li> <li>Enables Control Plane Logging.</li> <li>AWS Distro For OpenTelemetry Operator and Collector</li> <li>Logs with AWS for FluentBit and CloudWatch Logs</li> <li>Enables CloudWatch Container Insights.</li> <li>Installs Prometheus Node Exporter for infrastructure metrics.</li> </ul>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</p> </li> </ol> <pre><code>make build\nmake pattern single-new-eks-awsnative-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-awsnative-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-awsnative-singleneweksawsnativeobs-JN3QM2KMBNCO\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> Output:</p> <pre><code>NAME                                         STATUS   ROLES    AGE    VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-104-200.us-west-2.compute.internal   Ready    &lt;none&gt;   2d1h   v1.25.9-eks-0a21954   10.0.104.200   &lt;none&gt;        Amazon Linux 2   5.10.179-168.710.amzn2.x86_64   containerd://1.6.19\n</code></pre> <p>Next, lets verify the namespaces in the cluster:</p> <pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <p>Output:</p> <pre><code>NAME                       STATUS   AGE\namazon-cloudwatch          Active   5h36m\ncert-manager               Active   5h36m\ndefault                    Active   5h46m\nkube-node-lease            Active   5h46m\nkube-public                Active   5h46m\nkube-system                Active   5h46m\nprometheus-node-exporter   Active   5h36m\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#visualization","title":"Visualization","text":"<p>Navigate to CloudWatch and go to \"Container Insights\".</p> <p>View the Container Map:</p> <p></p> <p>View the Resource List:</p> <p></p> <p>View the Performance Monitoring Dashboard:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#viewing-logs","title":"Viewing Logs","text":"<p>Refer to \"Using CloudWatch Logs Insights to Query Logs in Logging.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#enabling-application-signals-for-your-services","title":"Enabling Application Signals for your services","text":"<p>Amazon CloudWatch Application Signals is a new integrated native APM experience in AWS. CloudWatch Application Signals supports Java and Python applications running on your Amazon EKS cluster.</p> <p>If you haven't enabled Application Signals in this account yet, follow steps 1 - 4 in our AWS documentation.</p> <p>Next, you have to update your Application to <code>Configure application metrics and trace sampling</code>. For this, you must add an annotation to a manifest YAML in your cluster. Adding this annotation auto-instruments the application to send metrics, traces, and logs to Application Signals. You have two options for the annotation:</p> <ol> <li> <p>Annotate Workload auto-instruments a single workload in the cluster.</p> <ul> <li>Paste the below line into the PodTemplate section of the workload manifest. <pre><code>apiVersion: apps/v1\nkind: Deployment\nspec:\n  template:\n    metadata:\n      # add this annotation under the pod template metadata of the services deployment YAML you want to monitor\n      annotations:\n        instrumentation.opentelemetry.io/inject-java: \"true\"\n        instrumentation.opentelemetry.io/inject-python: \"true\"\n...\n</code></pre></li> <li>In your terminal, enter <code>kubectl apply -f your_deployment_yaml</code> to apply the change.</li> </ul> </li> <li> <p>Annotate Namespace auto-instruments all workloads deployed in the selected namespace.</p> <ul> <li>Paste the below line into the metadata section of the namespace manifest. <pre><code>annotations: instrumentation.opentelemetry.io/inject-java: \"true\"\napiVersion: apps/v1\nkind: Namespace\nmetadata:\n    name: &lt;your_namespace&gt;\n    # add this annotation under metadata of the namespace manifest you want to monitor\n    annotations:\n      instrumentation.opentelemetry.io/inject-java: \"true\"\n      instrumentation.opentelemetry.io/inject-python: \"true\"\n...\n</code></pre></li> <li>In your terminal, enter <code>kubectl apply -f your_namespace_yaml</code> to apply the change.</li> <li>In your terminal, enter a command to restart all pods in the namespace. An example command to restart deployment workloads is <code>kubectl rollout restart deployment -n namespace_name</code></li> </ul> </li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#visualization-of-cloudwatch-application-signals-data","title":"Visualization of CloudWatch Application Signals data","text":"<p>After enabling your Application to pass metrics and traces by following the steps provided above, open your Amazon CloudWatch console in the same region as your EKS cluster, then from the left hand side choose <code>Application Signals -&gt; Services</code> and you will see the metrics shown on the sample dashboard below:</p> <p></p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-awsnative-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-awsnative-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cluster/","title":"Single Cluster Setup","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cluster/#objective","title":"Objective","text":"<p>This pattern deploys one production grade Amazon EKS cluster, without any Observability add-on.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cluster/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cluster/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</p> </li> </ol> <pre><code>make build\nmake pattern single-new-eks-cluster deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cluster/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-observabil-singleneweksobservabilit-5NW0A5AUXVS9\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> Output:</p> <pre><code>NAME                                            STATUS   ROLES    AGE     VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-157-151.eu-central-1.compute.internal   Ready    &lt;none&gt;   9m19s   v1.25.9-eks-0a21954   10.0.157.151   &lt;none&gt;        Amazon Linux 2   5.10.179-168.710.amzn2.x86_64   containerd://1.6.19\n</code></pre> <p>Next, lets verify the namespaces in the cluster:</p> <pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <p>Output:</p> <pre><code>NAME                       STATUS   AGE\ncert-manager               Active   7m8s\ndefault                    Active   13m\nexternal-secrets           Active   7m9s\nkube-node-lease            Active   13m\nkube-public                Active   13m\nkube-system                Active   13m\nprometheus-node-exporter   Active   7m9s\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cluster/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-cluster destroy\n</code></pre> <p>aws</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-container-logs-opensource-observability/","title":"Single Cluster Open Source Observability - Container Logs Collection","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-container-logs-opensource-observability/#objective","title":"Objective","text":"<p>Following the announcement of logs support in AWS Distro for OpenTelemetry, this pattern demonstrates how to use the New EKS Cluster Open Source Observability Accelerator to forward container logs to cloud watch using ADOT containers log collector.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-container-logs-opensource-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-container-logs-opensource-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern, except for step 7, where you need to replace \"context\" in <code>~/.cdk.json</code> with the following:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        }\n      ]\n    },\n    \"adotcontainerlogs.pattern.enabled\": true\n  }\n</code></pre> <p>!! warning This scenario might need larger worker node for the pod. </p> <p>Once completed the rest of the Deploying steps, you can move on with the deployment of the Nginx workload.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-container-logs-opensource-observability/#viewing-logs-in-cloudwatch-log-groups-and-logs-insights","title":"Viewing Logs in CloudWatch Log Groups and Logs Insights","text":"<p>Navigate to CloudWatch, then go to \"Log groups\"</p> <p>Search for log group with the name \"/aws/eks/single-new-eks-mixed-observability-accelerator\" and open it</p> <p>You will see log streams created using the node name</p> <p></p> <p>Open the log stream and you view the logs forwarded by the container logs collector to CloudWatch</p> <p></p> <p>Navigate to CloudWatch, then go to \"Logs Insights\"</p> <p>In the dropdown, select log group with name \"/aws/eks/single-new-eks-mixed-observability-accelerator\" and run a query.</p> <p></p> <p>Then you can view the results of your query:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-container-logs-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cost-monitoring-ingress-observability/","title":"Single Cluster Observability - Kubecost Cost Monitoring with Secure Ingress using Cognito","text":"<p>Implementing Kubecost for monitoring EKS clusters provides invaluable insights into resource utilization and cost management. Kubecost offers granular visibility into the cost breakdown of Kubernetes workloads, enabling efficient allocation of resources and optimization of infrastructure spending. By integrating with Amazon Managed Prometheus (AMP) and AWS services such as Application Load Balancer, Amazon Cognito, and Amazon Route 53, Kubecost ensures a comprehensive monitoring solution with secure access control mechanisms. With alerts and recording rules provided by Amazon Managed Service for Prometheus, teams can proactively identify and address potential issues, ensuring optimal performance and cost-effectiveness of EKS deployments. Kubecost's user-friendly dashboard and reporting capabilities empower organizations to make informed decisions, maximize resource efficiency, and maintain cost predictability in their EKS environments, ultimately enhancing operational excellence and driving business growth.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cost-monitoring-ingress-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS cost monitoring (Kubecost) pattern with Application Load Balancer, Amazon Cognito, and a Transport Layer Security (TLS) Certificate on AWS Certificate Manager (ACM) with Amazon Route 53 hosted zone to authenticate users to Kubecost</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cost-monitoring-ingress-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster.</li> <li>AWS Kubecost with Amazon Managed Prometheus (AMP) integration</li> <li>Secure Ingress with AWS Cognito</li> <li>AWS Certificate Manager with Amazon Route 53 hosted zone </li> <li>Alerts and recording rules with Amazon Managed Service for Prometheus</li> </ul>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cost-monitoring-ingress-observability/#prerequisites","title":"Prerequisites:","text":"<p>An existing hosted zone in Route53 with the ability to add records.</p> <p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cost-monitoring-ingress-observability/#configuring-domain","title":"Configuring domain","text":"<p>The CDK code expects the allowed domain and subdomain names in the CDK context file (cdk.json).</p> <p>Create two environment variables. The PARENT_HOSTED_ZONE variable contains the name of your Route 53 public hosted zone. The DEV_SUBZONE_NAME will be the address for your Kubecost dashboard.</p> <p>When users register to cognito they will have to provide an email address, using the <code>allowed.domains.list</code> you can specify you enterprise's email domain to only allow your employees to sign up for the service</p> <p>Generate the cdk.json file:</p> <pre><code>PARENT_HOSTED_ZONE=mycompany.a2z.com\nDEV_SUBZONE_NAME=kubecost.mycompany.a2z.com\nALLOWED_DOMAIN_LIST=amazon.com\ncat &lt;&lt; EOF &gt; cdk.json\n{\n    \"app\": \"npx ts-node dist/lib/common/default-main.js\",\n    \"context\": {\n        \"parent.hostedzone.name\": \"${PARENT_HOSTED_ZONE}\",\n        \"dev.subzone.name\": \"${DEV_SUBZONE_NAME}\",\n        \"allowed.domains.list\": \"${ALLOWED_DOMAIN_LIST}\"\n      }\n}\nEOF\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cost-monitoring-ingress-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern till step 7. At step 8, execute the following</p> <pre><code>make build\nmake pattern single-new-eks-cost-monitoring deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cost-monitoring-ingress-observability/#verifying-a-record-for-route53","title":"Verifying A record for Route53","text":"<p>Open the AWS console once the deployment is complete.  Navigate to Route53 in AWS console and select the hosted zone you used for the deployment.  Verify the entry of a record matching the DEV_SUBZONE_NAME we used.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cost-monitoring-ingress-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-fargate-opensource-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-fargate-op-singleneweksfargateopens-xxxxxxxx\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get pods -o wide -A\n</code></pre> <pre><code>Output:\nNAMESPACE                       NAME                                                              READY   STATUS    RESTARTS   AGE     IP             NODE                                         NOMINATED NODE   READINESS GATES\namazon-guardduty                aws-guardduty-agent-5lblf                                         1/1     Running   0          3h43m   10.0.184.135   ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\namazon-guardduty                aws-guardduty-agent-qzm4j                                         1/1     Running   0          3h43m   10.0.153.58    ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nargocd                          blueprints-addon-argocd-application-controller-0                  1/1     Running   0          3h40m   10.0.128.187   ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nargocd                          blueprints-addon-argocd-applicationset-controller-7d77d5cdjjhm8   1/1     Running   0          3h40m   10.0.148.136   ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nargocd                          blueprints-addon-argocd-dex-server-84dc54844f-lwgss               1/1     Running   0          3h40m   10.0.178.159   ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nargocd                          blueprints-addon-argocd-notifications-controller-597477df8q4btr   1/1     Running   0          3h40m   10.0.166.196   ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nargocd                          blueprints-addon-argocd-redis-79cb6b87dc-tddlm                    1/1     Running   0          3h40m   10.0.160.149   ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nargocd                          blueprints-addon-argocd-repo-server-584549c456-5gfs8              1/1     Running   0          3h40m   10.0.146.88    ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nargocd                          blueprints-addon-argocd-server-7b7b488dd4-686tx                   1/1     Running   0          3h40m   10.0.175.70    ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\naws-for-fluent-bit              blueprints-addon-aws-fluent-bit-for-cw-aws-for-fluent-bit-lr99l   1/1     Running   0          3h40m   10.0.160.194   ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\naws-for-fluent-bit              blueprints-addon-aws-fluent-bit-for-cw-aws-for-fluent-bit-z2pm7   1/1     Running   0          3h40m   10.0.146.233   ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\ncert-manager                    cert-manager-6d988558d6-wm746                                     1/1     Running   0          3h40m   10.0.188.100   ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\ncert-manager                    cert-manager-cainjector-6976895488-mk9sw                          1/1     Running   0          3h40m   10.0.173.79    ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\ncert-manager                    cert-manager-webhook-fcf48cc54-92wqm                              1/1     Running   0          3h40m   10.0.133.37    ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\ndefault                         otel-collector-amp-collector-6d768bcbf5-vbmqr                     1/1     Running   0          3h39m   10.0.171.253   ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nexternal-dns                    blueprints-addon-external-dns-78bcd6c7c5-df74q                    1/1     Running   0          3h40m   10.0.180.87    ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nexternal-secrets                blueprints-addon-external-secrets-675f847b97-kbn98                1/1     Running   0          3h40m   10.0.178.180   ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nexternal-secrets                blueprints-addon-external-secrets-cert-controller-68cbb65dspf8c   1/1     Running   0          3h40m   10.0.154.4     ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nexternal-secrets                blueprints-addon-external-secrets-webhook-6cfdbdf896-j9ng7        1/1     Running   0          3h40m   10.0.142.78    ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     aws-load-balancer-controller-7cd4b895d4-gvxtv                     1/1     Running   0          3h40m   10.0.131.188   ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     aws-load-balancer-controller-7cd4b895d4-m2dh5                     1/1     Running   0          3h40m   10.0.173.13    ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     aws-node-7l22p                                                    2/2     Running   0          3h43m   10.0.184.135   ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     aws-node-rfc76                                                    2/2     Running   0          3h43m   10.0.153.58    ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     blueprints-addon-metrics-server-7cb6564d98-jhwmj                  1/1     Running   0          3h40m   10.0.182.218   ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     blueprints-addon-secret-store-csi-driver-secrets-store-csi5fbjj   3/3     Running   0          3h40m   10.0.190.108   ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     blueprints-addon-secret-store-csi-driver-secrets-store-csigdgfd   3/3     Running   0          3h40m   10.0.148.234   ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     coredns-5b8cc885bc-t9dpp                                          1/1     Running   0          3h47m   10.0.132.167   ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     coredns-5b8cc885bc-tkq6g                                          1/1     Running   0          3h47m   10.0.152.126   ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     csi-secrets-store-provider-aws-ktklg                              1/1     Running   0          3h40m   10.0.190.207   ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     csi-secrets-store-provider-aws-qmg44                              1/1     Running   0          3h40m   10.0.142.192   ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     ebs-csi-controller-5c4b7b9549-cvv8b                               6/6     Running   0          3h40m   10.0.163.2     ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     ebs-csi-controller-5c4b7b9549-d9wfc                               6/6     Running   0          3h40m   10.0.146.91    ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     ebs-csi-node-9sxtr                                                3/3     Running   0          3h40m   10.0.155.48    ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     ebs-csi-node-bcsjk                                                3/3     Running   0          3h40m   10.0.187.96    ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     kube-proxy-djbgh                                                  1/1     Running   0          3h43m   10.0.153.58    ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     kube-proxy-mck62                                                  1/1     Running   0          3h43m   10.0.184.135   ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     kube-state-metrics-6cf6f65cf7-nzqkb                               1/1     Running   0          3h40m   10.0.186.50    ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     ssm-installer-fsfjn                                               1/1     Running   0          3h41m   10.0.189.79    ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     ssm-installer-vbqqm                                               1/1     Running   0          3h41m   10.0.154.228   ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nkubecost                        kubecost-cost-analyzer-5769d5f47f-fjwkz                           3/3     Running   0          3h40m   10.0.137.248   ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nkubecost                        kubecost-cost-analyzer-prometheus-server-6f48bdc56c-d6789         2/2     Running   0          3h40m   10.0.187.76    ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nopentelemetry-operator-system   opentelemetry-operator-98f5b9c89-7kp6x                            2/2     Running   0          3h39m   10.0.148.126   ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\nprometheus-node-exporter        prometheus-node-exporter-czsx8                                    1/1     Running   0          3h40m   10.0.184.135   ip-10-0-184-135.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nprometheus-node-exporter        prometheus-node-exporter-jg9tw                                    1/1     Running   0          3h40m   10.0.153.58    ip-10-0-153-58.us-west-2.compute.internal    &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>Now, lets navigate to the URL described as our dev.subzone.name in the cdk.json file and signup with a new cognito user profile.</p> <ul> <li>Kubecost Dashboards</li> </ul> <p></p> <ul> <li>Kubecost Namespace Dashboards</li> </ul> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-cost-monitoring-ingress-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-cost-monitoring destroy \n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/","title":"Single Cluster Opensource Observability - Fargate","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Fargate Open Source Observability pattern using open source tooling such as AWS Distro for Open Telemetry (ADOT), FluentBit (Logs), Amazon Managed Service for Prometheus and Amazon Managed Grafana:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#metrics-and-traces","title":"Metrics and Traces","text":"<p>AWS Distro for OpenTelemetry (ADOT) is a secure, AWS-supported distribution of the OpenTelemetry project. With ADOT, users can instrument their applications just once to send correlated metrics and traces to multiple monitoring solutions. </p> <p>The ADOT Collector has the concept of a pipeline which comprises three key types of components, namely, receiver, processor, and exporter. A receiver is how data gets into the collector. It accepts data in a specified format, translates it into the internal format and passes it to processors and exporters defined in the pipeline. It can be pull or push based. A processor is an optional component that is used to perform tasks such as batching, filtering, and transformations on data between being received and being exported. An exporter is used to determine which destination to send the metrics, logs or traces.</p> <p>In the above architecture, the kubelet on a worker node in a Kubernetes cluster exposes resource metrics such as CPU, memory, disk, and network usage at the /metrics/cadvisor endpoint. However, in EKS Fargate networking architecture, a pod is not allowed to directly reach the kubelet on that worker node. Hence, the ADOT Collector calls the Kubernetes API Server to proxy the connection to the kubelet on a worker node, and collect kubelet's cAdvisor metrics for workloads on that node. These metrics are made available in Prometheus format. Therefore, the collector uses an instance of Prometheus Receiver as a drop-in replacement for a Prometheus server and scrapes these metrics from the Kubernetes API server endpoint. Using Kubernetes service discovery, the receiver can discover all the worker nodes in an EKS cluster. Hence, more than one instances of ADOT Collector will suffice to collect resource metrics from all the nodes in a cluster. Having a single instance of ADOT collector can be overwhelming during higher loads so always recommend to deploy more than one collector.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#logs","title":"Logs","text":"<p>With Amazon EKS on Fargate, you can deploy pods without allocating or managing your Kubernetes nodes. This removes the need to capture system-level logs for your Kubernetes nodes. To capture the logs from your Fargate pods, we use Fluent Bit to forward the logs directly to CloudWatch. This enables you to automatically route logs to CloudWatch without further configuration or a sidecar container for your Amazon EKS pods on Fargate. For more information about this, see Fargate logging in the Amazon EKS documentation and Fluent Bit for Amazon EKS on the AWS Blog. This solution captures the STDOUT and STDERR input/output (I/O) streams from your container and sends them to CloudWatch through Fluent Bit, based on the Fluent Bit configuration established for the Amazon EKS cluster on Fargate</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster.</li> <li>AWS Distro For OpenTelemetry Operator and Collector for Metrics and Traces</li> <li>Logs with AWS for FluentBit</li> <li>Installs Grafana Operator to add AWS data sources and create Grafana Dashboards to Amazon Managed Grafana.</li> <li>Installs FluxCD to perform GitOps sync of a Git Repo to EKS Cluster. We will use this later for creating Grafana Dashboards and AWS datasources to Amazon Managed Grafana. You can also use your own GitRepo  to sync your own Grafana resources such as Dashboards, Datasources etc. Please check our One observability module - GitOps with Amazon Managed Grafana to learn more about this.</li> <li>Installs External Secrets Operator to retrieve and Sync the Grafana API keys.</li> <li>Amazon Managed Grafana Dashboard and data source</li> <li>Alerts and recording rules with Amazon Managed Service for Prometheus</li> </ul>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern till step 7. At step 8, execute the following</p> <pre><code>make build\nmake pattern single-new-eks-fargate-opensource-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-fargate-opensource-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-fargate-op-singleneweksfargateopens-xxxxxxxx\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> <pre><code>Output:\nNAME                                   STATUS   ROLES    AGE   VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nfargate-ip-10-0-100-154.ec2.internal   Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.100.154   &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-102-67.ec2.internal    Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.102.67    &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-121-124.ec2.internal   Ready    &lt;none&gt;   7d11h   v1.27.1-eks-2f008fe   10.0.121.124   &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-135-174.ec2.internal   Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.135.174   &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-135-90.ec2.internal    Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.135.90    &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-136-121.ec2.internal   Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.136.121   &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-138-15.ec2.internal    Ready    &lt;none&gt;   2d8h    v1.27.6-eks-f8587cb   10.0.138.15    &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-139-227.ec2.internal   Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.139.227   &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-148-152.ec2.internal   Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.148.152   &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-151-22.ec2.internal    Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.151.22    &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-158-82.ec2.internal    Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.158.82    &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-161-151.ec2.internal   Ready    &lt;none&gt;   2d8h    v1.27.6-eks-f8587cb   10.0.161.151   &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-164-251.ec2.internal   Ready    &lt;none&gt;   87m     v1.27.6-eks-f8587cb   10.0.164.251   &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-165-99.ec2.internal    Ready    &lt;none&gt;   2d9h    v1.27.1-eks-2f008fe   10.0.165.99    &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-167-115.ec2.internal   Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.167.115   &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-178-170.ec2.internal   Ready    &lt;none&gt;   12d     v1.27.1-eks-2f008fe   10.0.178.170   &lt;none&gt;        Amazon Linux 2   5.10.192-182.736.amzn2.x86_64   containerd://1.6.6\nfargate-ip-10-0-186-44.ec2.internal    Ready    &lt;none&gt;   87m     v1.27.6-eks-f8587cb   10.0.186.44    &lt;none&gt;        Amazon Linux 2   5.10.192-183.736.amzn2.x86_64   containerd://1.6.6\n</code></pre></p> <p><pre><code>kubectl get pods -o wide -A\n</code></pre> <pre><code>NAMESPACE                       NAME                                                              READY   STATUS    RESTARTS        AGE     IP             NODE                                   NOMINATED NODE   READINESS GATES\ncert-manager                    cert-manager-8694c7d4fd-pwmhh                                     1/1     Running   0               12d     10.0.135.90    fargate-ip-10-0-135-90.ec2.internal    &lt;none&gt;           &lt;none&gt;\ncert-manager                    cert-manager-cainjector-744cb68868-m2j25                          1/1     Running   0               12d     10.0.136.121   fargate-ip-10-0-136-121.ec2.internal   &lt;none&gt;           &lt;none&gt;\ncert-manager                    cert-manager-webhook-5f6fff764b-4nq5q                             1/1     Running   0               12d     10.0.151.22    fargate-ip-10-0-151-22.ec2.internal    &lt;none&gt;           &lt;none&gt;\ndefault                         otel-collector-amp-collector-7cc9cfb77f-kjp5b                     1/1     Running   0               2d9h    10.0.165.99    fargate-ip-10-0-165-99.ec2.internal    &lt;none&gt;           &lt;none&gt;\nexternal-secrets                blueprints-addon-external-secrets-797c97cc56-qnqvb                1/1     Running   0               12d     10.0.189.201   fargate-ip-10-0-189-201.ec2.internal   &lt;none&gt;           &lt;none&gt;\nexternal-secrets                blueprints-addon-external-secrets-cert-controller-75ccc646775f6   1/1     Running   0               12d     10.0.100.154   fargate-ip-10-0-100-154.ec2.internal   &lt;none&gt;           &lt;none&gt;\nexternal-secrets                blueprints-addon-external-secrets-webhook-749d46f5df-slb88        1/1     Running   0               12d     10.0.189.119   fargate-ip-10-0-189-119.ec2.internal   &lt;none&gt;           &lt;none&gt;\nflux-system                     helm-controller-69ff5c96c7-xkbpc                                  1/1     Running   0               12d     10.0.190.34    fargate-ip-10-0-190-34.ec2.internal    &lt;none&gt;           &lt;none&gt;\nflux-system                     image-automation-controller-65887476b7-8tvl6                      1/1     Running   0               12d     10.0.167.115   fargate-ip-10-0-167-115.ec2.internal   &lt;none&gt;           &lt;none&gt;\nflux-system                     image-reflector-controller-57847dc9cf-6pbts                       1/1     Running   0               12d     10.0.178.170   fargate-ip-10-0-178-170.ec2.internal   &lt;none&gt;           &lt;none&gt;\nflux-system                     kustomize-controller-68c6c766-hrxh4                               1/1     Running   0               12d     10.0.102.67    fargate-ip-10-0-102-67.ec2.internal    &lt;none&gt;           &lt;none&gt;\nflux-system                     notification-controller-5dbc9fc9c4-b7gvt                          1/1     Running   0               12d     10.0.188.107   fargate-ip-10-0-188-107.ec2.internal   &lt;none&gt;           &lt;none&gt;\nflux-system                     source-controller-5b669588f-jtgc6                                 1/1     Running   0               12d     10.0.148.152   fargate-ip-10-0-148-152.ec2.internal   &lt;none&gt;           &lt;none&gt;\ngrafana-operator                grafana-operator-7d7ccc88f4-fw99n                                 1/1     Running   0               12d     10.0.99.84     fargate-ip-10-0-99-84.ec2.internal     &lt;none&gt;           &lt;none&gt;\nkube-system                     aws-load-balancer-controller-7c7f88558d-rlzh6                     1/1     Running   0               12d     10.0.158.82    fargate-ip-10-0-158-82.ec2.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     aws-load-balancer-controller-7c7f88558d-v797p                     1/1     Running   1 (5h40m ago)   12d     10.0.190.41    fargate-ip-10-0-190-41.ec2.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     blueprints-addon-metrics-server-6765c9bc59-85jqq                  1/1     Running   0               90m     10.0.186.44    fargate-ip-10-0-186-44.ec2.internal    &lt;none&gt;           &lt;none&gt;\nkube-system                     coredns-6549dc85b9-586mh                                          1/1     Running   0               12d     10.0.139.227   fargate-ip-10-0-139-227.ec2.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     coredns-6549dc85b9-n6xdr                                          1/1     Running   0               12d     10.0.135.174   fargate-ip-10-0-135-174.ec2.internal   &lt;none&gt;           &lt;none&gt;\nkube-system                     kube-state-metrics-596b5dbf46-c9mdt                               1/1     Running   0               90m     10.0.164.251   fargate-ip-10-0-164-251.ec2.internal   &lt;none&gt;           &lt;none&gt;\nopentelemetry-operator-system   opentelemetry-operator-5ddbdcdc57-nh5dr                           2/2     Running   0               7d11h   10.0.121.124   fargate-ip-10-0-121-124.ec2.internal   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p><pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <pre><code>NAME                       STATUS   AGE\naws-for-fluent-bit              Active   12d\ncert-manager                    Active   12d\ndefault                         Active   12d\nexternal-secrets                Active   12d\nflux-system                     Active   12d\ngrafana-operator                Active   12d\nkube-node-lease                 Active   12d\nkube-public                     Active   12d\nkube-system                     Active   12d\nopentelemetry-operator-system   Active   7d11h\n</code></pre></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#visualize-dashboards-in-amazon-managed-grafana","title":"Visualize Dashboards in Amazon Managed Grafana","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a number of dashboards under <code>Observability Accelerator Dashboards</code>. Open <code>Kubernetes / Compute Resources / Cluster</code> and <code>Kubernetes / Kubelet</code> Dashboards, you should see data as below</p> <ul> <li>Cluster Dashboards</li> </ul> <p></p> <p>You can also visualize cluster metrics for specific namespaces by clicking on a particular namespace. For instance, below is a snapshot of the <code>kube-system</code> namespace.</p> <p></p> <ul> <li>Kubelet Dashboard</li> </ul> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#deploy-sample-java-workload","title":"Deploy Sample Java Workload","text":"<p>We'll deploy a sample java workload in our newly created EKS cluster running on AWS Fargate. To do that, follow the instructions in New EKS Cluster Java Open Source Observability Accelerator pattern till step number 4.</p> <p>Since we're deploying the sample workload on AWS Fargate compute, we need to create a Fargate profile for running the java application.</p> <p>Execute the following command to create a Fargate profile</p> <pre><code>SAMPLE_TRAFFIC_NAMESPACE=javajmx-sample\nCLUSTER_NAME=single-new-eks-fargate-opensource-observability-accelerator\neksctl create fargateprofile --namespace $SAMPLE_TRAFFIC_NAMESPACE --cluster $CLUSTER_NAME --name sample-java-workload-profile\n</code></pre> <p>Check if the profile got created successfully by running</p> <pre><code>$ eksctl get fargateprofile --cluster $CLUSTER_NAME\nNAME                                                            SELECTOR_NAMESPACE              SELECTOR_LABELS POD_EXECUTION_ROLE_ARN                                                                          SUBNETS          TAGS     STATUS\nsample-java-workload-profile                                    javajmx-sample                  &lt;none&gt;          arn:aws:iam::200202725330:role/eksctl-single-new-eks-farga-FargatePodExecutionRole-pBT3sLM15PYx subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\nsingleneweksfargateopensourceo-153ba837dcf44bbe84881aa5336f0bf1 default                         &lt;none&gt;          arn:aws:iam::200202725330:role/single-new-eks-fargate-op-singleneweksfargateopens-1UCAAI02CIG27 subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\nsingleneweksfargateopensourceo-153ba837dcf44bbe84881aa5336f0bf1 kube-system                     &lt;none&gt;          arn:aws:iam::200202725330:role/single-new-eks-fargate-op-singleneweksfargateopens-1UCAAI02CIG27 subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\nsingleneweksfargateopensourceo-8faabc1806d44e4bb0f656aa6785e276 cert-manager                    &lt;none&gt;          arn:aws:iam::200202725330:role/single-new-eks-fargate-op-singleneweksfargateopens-FDF881YCTAAT  subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\nsingleneweksfargateopensourceo-8faabc1806d44e4bb0f656aa6785e276 external-secrets                &lt;none&gt;          arn:aws:iam::200202725330:role/single-new-eks-fargate-op-singleneweksfargateopens-FDF881YCTAAT  subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\nsingleneweksfargateopensourceo-8faabc1806d44e4bb0f656aa6785e276 flux-system                     &lt;none&gt;          arn:aws:iam::200202725330:role/single-new-eks-fargate-op-singleneweksfargateopens-FDF881YCTAAT  subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\nsingleneweksfargateopensourceo-8faabc1806d44e4bb0f656aa6785e276 grafana-operator                &lt;none&gt;          arn:aws:iam::200202725330:role/single-new-eks-fargate-op-singleneweksfargateopens-FDF881YCTAAT  subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\nsingleneweksfargateopensourceo-8faabc1806d44e4bb0f656aa6785e276 opentelemetry-operator-system   &lt;none&gt;          arn:aws:iam::200202725330:role/single-new-eks-fargate-op-singleneweksfargateopens-FDF881YCTAAT  subnet-0d9ae01b957d93732,subnet-07e7079de2ee19ee1,subnet-0d1f688becf9864ff        &lt;none&gt;  ACTIVE\n</code></pre> <p>Now run step 5 of the instructions at New EKS Cluster Java Open Source Observability Accelerator pattern.</p> <p>Check whether the Sample Java Workload got deployed successfully</p> <pre><code>kubectl get po -n $SAMPLE_TRAFFIC_NAMESPACE\n\nNAME                             READY   STATUS    RESTARTS   AGE\ntomcat-bad-traffic-generator     1/1     Running   0          2d9h\ntomcat-example-fcbb8856b-s4mq8   1/1     Running   0          2d9h\ntomcat-traffic-generator         1/1     Running   0          2d9h\n</code></pre> <p>You should now see a new dashboard named <code>Java/JMX</code>, under <code>Observability Accelerator Dashboards</code>:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#viewing-logs","title":"Viewing Logs","text":"<p>Amazon EKS on Fargate offers a built-in log router based on Fluent Bit. This means that you don't explicitly run a Fluent Bit container as a sidecar, but Amazon runs it for you. All that you have to do is configure the log router. The configuration happens through a dedicated <code>ConfigMap</code>. Logs are collected and exported to Amazon CloudWatch Logs, which enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. By default, the logs are exported to us-east-1 region but you can modify the <code>ConfigMap</code> for your region of choice. At least one supported <code>OUTPUT</code> plugin has to be provided in the <code>ConfigMap</code> to enable logging. You can also modify the destination from cloudwatch to Cloudwatch (default), Amazon OpenSearch Service or Kinesis Data Firehose. Read more about EKS Fargate logging.</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-fargate-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-fargate-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/","title":"Single Cluster Open Source Observability - NVIDIA GPU","text":"<p>Graphics Processing Units (GPUs) play an integral part in the Machine Learning (ML) workflow, by providing the scalable performance needed for fast ML training and cost-effective ML inference. On top of that, they are used in flexible remote virtual workstations and powerful HPC computations.</p> <p>This pattern shows you how to monitor the performance of the GPUs units, used in an Amazon EKS cluster leveraging GPU-based instances.</p> <p>Amazon Managed Service for Prometheus and Amazon Managed Grafana are open source tools used in this pattern to collect and visualise metrics respectively.</p> <p>Amazon Managed Service for Prometheus is a Prometheus-compatible service that monitors and provides alerts on containerized applications and infrastructure at scale.</p> <p>Amazon Managed Grafana is a managed service for Grafana, a popular open-source analytics platform that enables you to query, visualize, and alert on your metrics, logs, and traces.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/#objective","title":"Objective","text":"<p>This pattern deploys an Amazon EKS cluster and a node group that includes instance types featuring NVIDIA GPUs.</p> <p>The AMI type of the node group is <code>AL2_x86_64_GPU AMI</code>, which uses the Amazon EKS-optimized Linux AMI with GPU support. In addition to the standard Amazon EKS-optimized AMI configuration, the GPU AMI includes the NVIDIA drivers.</p> <p>The NVIDIA Data Center GPU Manager (DCGM) is a suite of tools for managing and monitoring NVIDIA datacenter GPUs in cluster environments. It includes health monitoring, diagnostics, system alerts and governance policies. GPU metrics are exposed to Amazon Managed Service for Prometheus by the DCGM Exporter, that uses the Go bindings to collect GPU telemetry data from DCGM and then exposes the metrics for Amazon Managed Service for Prometheus to pull from, using an http endpoint (<code>/metrics</code>).</p> <p>The pattern deploys the NVIDIA GPU Operator add-on. The GPU Operator uses the NVIDIA DCGM Exporter to expose GPU telemetry to Amazon Managed Service for Prometheus.</p> <p>Data is visualised in Amazon Managed Grafana by the NVIDIA DCGM Exporter Dashboard.</p> <p>The rest of the setup to collect and visualise metrics with Amazon Managed Service for Prometheus and Amazon Managed Grafana, is similar to that used in other open-source based patterns included in this repository.</p> <p>It also enables control plane logging on the EKS cluster for a comprehensive overview of cluster health.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Amazon Managed Grafana</li> </ol> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>Warning</p> <p>Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens.</li> </ol> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository. </p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/gpu\"\n        }\n      ]\n    },\n    \"gpuNodeGroup\": {\n      \"instanceType\": \"g4dn.xlarge\",\n      \"desiredSize\": 2, \n      \"minSize\": 2, \n      \"maxSize\": 3,\n      \"ebsSize\": 50\n    },\n  }\n</code></pre> <p>Note: insure your selected instance type is available in your region. To check that, you can run the following command (amend <code>Values</code> below as you see fit):</p> <pre><code>aws ec2 describe-instance-type-offerings \\\n    --filters Name=instance-type,Values=\"g4*\" \\\n    --query \"InstanceTypeOfferings[].InstanceType\" \\\n    --region us-east-2\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern single-new-eks-gpu-opensource-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-opensource-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-opensource-singleneweksgpuopensourc...\n</code></pre> <p>Let\u2019s verify the resources created by steps above:</p> <pre><code>kubectl get pods -A\n</code></pre> <p>Output:</p> <p></p> <p>Next, let's verify that each node has allocatable GPUs:</p> <pre><code>kubectl get nodes  \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n</code></pre> <p>Output:</p> <p></p> <p>We can now deploy the <code>nvidia-smi</code> binary, which shows diagnostic information about all GPUs visible to the container:</p> <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nvidia-smi\nspec:\n  restartPolicy: OnFailure\n  containers:\n  - name: nvidia-smi\n    image: \"nvidia/cuda:11.0.3-base-ubuntu20.04\"\n    args:\n    - \"nvidia-smi\"\n    resources:\n      limits:\n        nvidia.com/gpu: 1\nEOF\n</code></pre> <p>Then request the logs from the Pod:</p> <pre><code>kubectl logs nvidia-smi\n</code></pre> <p>Output:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/#visualization","title":"Visualization","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/#grafana-nvidia-dcgm-exporter-dashboard","title":"Grafana NVIDIA DCGM Exporter Dashboard","text":"<p>Login to your Amazon Managed Grafana workspace and navigate to the Dashboards panel. You should see a dashboard named <code>NVIDIA DCGM Exporter Dashboard</code>.</p> <p>We will now generate some load, to see some metrics in the dashboard. Please run the following command from terminal:</p> <pre><code>cat &lt;&lt; EOF | kubectl create -f -\n apiVersion: v1\n kind: Pod\n metadata:\n   name: dcgmproftester\n spec:\n   restartPolicy: OnFailure\n   containers:\n   - name: dcgmproftester11\n     image: nvidia/samples:dcgmproftester-2.0.10-cuda11.0-ubuntu18.04\n     args: [\"--no-dcgm-validation\", \"-t 1004\", \"-d 120\"]\n     resources:\n       limits:\n          nvidia.com/gpu: 1\n     securityContext:\n       capabilities:\n          add: [\"SYS_ADMIN\"]\nEOF\n</code></pre> <p>To verify the Pod was successfully deployed, please run:</p> <pre><code>kubectl get pods\n</code></pre> <p>Expected output:</p> <p></p> <p>After a few minutes, looking into the <code>NVIDIA DCGM Exporter Dashboard</code>, you should see the gathered metrics, similar to: </p> <p></p> <p></p> <p></p> <p>Grafana Operator and Flux always work together to synchronize your dashboards with Git. If you delete your dashboards by accident, they will be re-provisioned automatically.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-gpu-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-gpu-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/","title":"Single Cluster Open Source Observability - Graviton","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Cluster Open Source Observability on Graviton pattern using open source tooling such as AWS Distro for Open Telemetry (ADOT), Amazon Managed Service for Prometheus and Amazon Managed Grafana:</p> <p></p> <p>Monitoring Amazon Elastic Kubernetes Service (Amazon EKS) for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#graviton","title":"Graviton","text":"<p>AWS Graviton Processors are designed by AWS to deliver the best price to performance for your cloud workloads running in Amazon EC2.  These processors are ARM chips running on aarch64 architecture. These processors feature key capabilities, such as the AWS Nitro System, that allow you to securely run cloud native applications at scale.</p> <p>Visit our EKS Blueprints docs for a list of supported addons on Graviton.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster running on a Graviton3 Processor</li> <li>Enables control plane logging</li> <li>AWS Distro For OpenTelemetry Operator and Collector for Metrics and Traces</li> <li>Logs with AWS for FluentBit</li> <li>Installs Grafana Operator to add AWS data sources and create Grafana Dashboards to Amazon Managed Grafana.</li> <li>Installs FluxCD to perform GitOps sync of a Git Repo to EKS Cluster. We will use this later for creating Grafana Dashboards and AWS datasources to Amazon Managed Grafana. You can also use your own GitRepo  to sync your own Grafana resources such as Dashboards, Datasources etc. Please check our One observability module - GitOps with Amazon Managed Grafana to learn more about this.</li> <li>Installs External Secrets Operator to retrieve and Sync the Grafana API keys.</li> <li>Amazon Managed Grafana Dashboard and data source</li> <li>Alerts and recording rules with Amazon Managed Service for Prometheus</li> </ul>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>Warning</p> <p>Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens.</li> </ol> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS Secrets Manager for GRAFANA API KEY: Update the Grafana API key secret in AWS Secrets using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws secretsmanager create-secret \\\n    --name grafana-api-key \\\n    --description \"API Key of your Grafana Instance\" \\\n    --secret-string \"${AMG_API_KEY}\" \\\n    --region $AWS_REGION \\\n    --query ARN \\\n    --output text\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository. </p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory. </p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        }\n      ]\n    },\n  }\n</code></pre> <p>If you need Java observability you can instead use:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true\n  }\n</code></pre> <p>If you want to deploy API Server dashboards along with Java observability you can instead use:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\",\n        \"GRAFANA_APISERVER_BASIC_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-basic.json\",\n        \"GRAFANA_APISERVER_ADVANCED_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-advanced.json\",\n        \"GRAFANA_APISERVER_TROUBLESHOOTING_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/apiserver/apiserver-troubleshooting.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/apiserver\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true,\n    \"apiserver.pattern.enabled\": true,\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern single-new-eks-graviton-opensource-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-graviton-opensource-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-gravitonop-singleneweksgravitonopens-82N8N3BMJYYI\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> Output:</p> <pre><code>NAME                                         STATUS   ROLES    AGE    VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-104-200.us-west-2.compute.internal   Ready    &lt;none&gt;   2d1h   v1.27.1-eks-2f008fe   10.0.104.200   &lt;none&gt;        Amazon Linux 2   5.10.179-168.710.amzn2.aarch64   containerd://1.6.19\n</code></pre> <p>Next, lets verify the namespaces in the cluster:</p> <pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <p>Output:</p> <pre><code>NAME                            STATUS   AGE\ncert-manager                    Active   2d1h\ndefault                         Active   2d1h\nexternal-secrets                Active   2d1h\nflux-system                     Active   2d1h\ngrafana-operator                Active   2d1h\nkube-node-lease                 Active   2d1h\nkube-public                     Active   2d1h\nkube-system                     Active   2d1h\nopentelemetry-operator-system   Active   2d1h\nprometheus-node-exporter        Active   2d1h\n</code></pre> <p>Next, lets verify all resources of <code>grafana-operator</code> namespace:</p> <pre><code>kubectl get all --namespace=grafana-operator\n</code></pre> <p>Output:</p> <pre><code>NAME                                    READY   STATUS    RESTARTS   AGE\npod/grafana-operator-866d4446bb-g5srl   1/1     Running   0          2d1h\n\nNAME                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/grafana-operator-metrics-service   ClusterIP   172.20.223.125   &lt;none&gt;        9090/TCP   2d1h\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grafana-operator   1/1     1            1           2d1h\n\nNAME                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grafana-operator-866d4446bb   1         1         1       2d1h\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#visualization","title":"Visualization","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#1-grafana-dashboards","title":"1. Grafana dashboards","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a list of dashboards under the <code>Observability Accelerator Dashboards</code></p> <p></p> <p>Open the <code>Cluster</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Namespace (Workloads)</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Node (Pods)</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Workload</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Kubelet</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Nodes</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>From the cluster to view all dashboards as Kubernetes objects, run:</p> <pre><code>kubectl get grafanadashboards -A\n</code></pre> <pre><code>NAMESPACE          NAME                                   AGE\ngrafana-operator   cluster-grafanadashboard               138m\ngrafana-operator   java-grafanadashboard                  143m\ngrafana-operator   kubelet-grafanadashboard               13h\ngrafana-operator   namespace-workloads-grafanadashboard   13h\ngrafana-operator   nginx-grafanadashboard                 134m\ngrafana-operator   node-exporter-grafanadashboard         13h\ngrafana-operator   nodes-grafanadashboard                 13h\ngrafana-operator   workloads-grafanadashboard             13h\n</code></pre> <p>You can inspect more details per dashboard using this command</p> <pre><code>kubectl describe grafanadashboards cluster-grafanadashboard -n grafana-operator\n</code></pre> <p>Grafana Operator and Flux always work together to synchronize your dashboards with Git. If you delete your dashboards by accident, they will be re-provisioned automatically.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#viewing-logs","title":"Viewing Logs","text":"<p>Refer to the \"Using CloudWatch Logs as a data source in Grafana\" section in Logging.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-graviton-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#troubleshooting","title":"Troubleshooting","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-graviton-opensource-observability/#1-grafana-dashboards-missing-or-grafana-api-key-expired","title":"1. Grafana dashboards missing or Grafana API key expired","text":"<p>In case you don't see the grafana dashboards in your Amazon Managed Grafana console, check on the logs on your grafana operator pod using the below command :</p> <pre><code>kubectl get pods -n grafana-operator\n</code></pre> <p>Output:</p> <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\ngrafana-operator-866d4446bb-nqq5c   1/1     Running   0          3h17m\n</code></pre> <pre><code>kubectl logs grafana-operator-866d4446bb-nqq5c -n grafana-operator\n</code></pre> <p>Output:</p> <pre><code>1.6857285045556655e+09  ERROR   error reconciling datasource    {\"controller\": \"grafanadatasource\", \"controllerGroup\": \"grafana.integreatly.org\", \"controllerKind\": \"GrafanaDatasource\", \"GrafanaDatasource\": {\"name\":\"grafanadatasource-sample-amp\",\"namespace\":\"grafana-operator\"}, \"namespace\": \"grafana-operator\", \"name\": \"grafanadatasource-sample-amp\", \"reconcileID\": \"72cfd60c-a255-44a1-bfbd-88b0cbc4f90c\", \"datasource\": \"grafanadatasource-sample-amp\", \"grafana\": \"external-grafana\", \"error\": \"status: 401, body: {\\\"message\\\":\\\"Expired API key\\\"}\\n\"}\ngithub.com/grafana-operator/grafana-operator/controllers.(*GrafanaDatasourceReconciler).Reconcile\n</code></pre> <p>If you observe, the the above <code>grafana-api-key error</code> in the logs, your grafana API key is expired. Please use the operational procedure to update your <code>grafana-api-key</code> :</p> <ul> <li>First, lets create a new Grafana API key.</li> </ul> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport GO_AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export GO_AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ul> <li>Finally, update the Grafana API key secret in AWS Secrets Manager using the above new Grafana API key:</li> </ul> <pre><code>export API_KEY_SECRET_NAME=\"grafana-api-key\"\naws secretsmanager update-secret \\\n    --secret-id $API_KEY_SECRET_NAME \\\n    --secret-string \"${AMG_API_KEY}\" \\\n    --region $AWS_REGION\n</code></pre> <ul> <li>If the issue persists, you can force the synchronization by deleting the <code>externalsecret</code> Kubernetes object.</li> </ul> <pre><code>kubectl delete externalsecret/external-secrets-sm -n grafana-operator\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-inferentia-opensource-observability/","title":"Single Cluster Open Source Observability - Inferentia-based cluster","text":"<p>AWS Inferentia is an accelerated Machine Learning (ML) chip, designed by AWS.</p> <p>Amazon Elastic Compute Cloud (Amazon EC2) Inf1 and Inf2 instances feature AWS Inferentia chips and support high-performance and low-latency inference.</p> <p>AWS Neuron is an SDK with a compiler, runtime, and profiling tools that helps developers deploy models on both AWS Inferentia accelerators and train them on AWS Trainium chips. It integrates natively with popular ML frameworks, such as PyTorch and TensorFlow.</p> <p>This pattern shows you how to monitor the performance of ML chips, used in an Amazon EKS cluster running on Amazon EC2 Inf1 and Inf2 instances.</p> <p>Amazon Managed Service for Prometheus and Amazon Managed Grafana are open source tools used in this pattern to collect and visualise metrics respectively.</p> <p>Amazon Managed Service for Prometheus is a Prometheus-compatible service that monitors and provides alerts on containerized applications and infrastructure at scale.</p> <p>Amazon Managed Grafana is a managed service for Grafana, a popular open-source analytics platform that enables you to query, visualize, and alert on your metrics, logs, and traces.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-inferentia-opensource-observability/#objective","title":"Objective","text":"<p>This pattern deploys an Amazon EKS cluster with a node group that includes Inf1/Inf2 instances.</p> <p>The AMI type of the node group is <code>AL2_x86_64_GPU AMI</code>, which uses the Amazon EKS-optimized accelerated AMI. In addition to the standard Amazon EKS-optimized AMI configuration, the accelerated AMI includes the NeuronX container runtime.</p> <p>To access the ML chips from Kubernetes, the pattern deploys the Neuron device plugin.</p> <p>Metrics are exposed to Amazon Managed Service for Prometheus by the <code>neuron-monitor</code> DaemonSet, which deploys a minimal container, with the Neuron Tools installed. Specifically, the <code>neuron-monitor</code> DaemonSet runs the <code>neuron-monitor</code> command piped into the <code>neuron-monitor-prometheus.py</code> companion script (both commands are part of the container):</p> <pre><code>neuron-monitor | neuron-monitor-prometheus.py --port &lt;port&gt;\n</code></pre> <p><code>neuron-monitor</code> collects metrics and stats from the Neuron Applications running on the system and streams the collected data to stdout in JSON format.</p> <p><code>neuron-monitor-prometheus.py</code> maps and exposes the telemetry data from JSON format into Prometheus compatible format.</p> <p>Data is visualised in Amazon Managed Grafana by the corresponding dashboard.</p> <p>The rest of the setup to collect and visualise metrics with Amazon Managed Service for Prometheus and Amazon Managed Grafana, is similar to that used in other open-source based patterns, included in this repository.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-inferentia-opensource-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-inferentia-opensource-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Amazon Managed Grafana</li> </ol> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>Warning</p> <p>Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens.</li> </ol> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository. </p> </li> <li> <p>The actual settings for dashboard urls and EKS managed node group are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory.</p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_NEURON_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/neuron/neuron-monitor.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/neuron\"\n        }\n      ]\n    },\n     \"neuronNodeGroup\": {\n      \"instanceClass\": \"inf1\",\n      \"instanceSize\": \"2xlarge\",\n      \"desiredSize\": 1, \n      \"minSize\": 1, \n      \"maxSize\": 3,\n      \"ebsSize\": 512\n    },\n  }\n</code></pre> <p>Note: you can replace the inf1 instance type with inf2 and the size as you prefer; to check availability in your selected Region, you can run the following command (amend <code>Values</code> below as you see fit):</p> <pre><code>aws ec2 describe-instance-type-offerings \\\n    --filters Name=instance-type,Values=\"inf1*\" \\\n    --query \"InstanceTypeOfferings[].InstanceType\" \\\n    --region $AWS_REGION\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern single-new-eks-inferentia-opensource-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-inferentia-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-inferentia-opensource... --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-inferentia-opensource-singleneweksgpuopensourc...\n</code></pre> <p>Let\u2019s verify the resources created by steps above:</p> <pre><code>kubectl get pods -A\n</code></pre> <p>Output:</p> <p></p> <p>Specifically, ensure <code>neuron-device-plugin-daemonset</code> DaemonSet is running:</p> <pre><code>kubectl get ds neuron-device-plugin-daemonset --namespace kube-system\n</code></pre> <p>Output:</p> <pre><code>NAME                             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\nneuron-device-plugin-daemonset   1         1         1       1            1           &lt;none&gt;          2h\n</code></pre> <p>And also that <code>neuron-monitor</code> DaemonSet is running:</p> <pre><code>kubectl get ds neuron-monitor --namespace kube-system\n</code></pre> <p>Output:</p> <pre><code>NAME             DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\nneuron-monitor   1         1         1       1            1           &lt;none&gt;          2h\n</code></pre> <p>Next, let's verify that Neuron devices and cores are visible, by running <code>neuron-ls</code> and <code>neuron-top</code> commands from e.g. your neuron-monitor pod:</p> <pre><code>kubectl exec -it {your neuron-monitor pod} -n kube-system -- /bin/bash -c \"neuron-ls\"\n</code></pre> <p>Output:</p> <p></p> <pre><code>kubectl exec -it {your neuron-monitor pod} -n kube-system -- /bin/bash -c \"neuron-top\"\n</code></pre> <p>Output:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-inferentia-opensource-observability/#visualization","title":"Visualization","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-inferentia-opensource-observability/#grafana-neuron-dashboard","title":"Grafana Neuron Dashboard","text":"<p>Login to your Amazon Managed Grafana workspace and navigate to the Dashboards panel. You should see a dashboard named <code>Neuron / Monitor</code>.</p> <p>To actually see some interesting metrics on the Grafana dashboard, we will apply the following manifest:</p> <pre><code>curl https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/k8s-deployment-manifest-templates/neuron/pytorch-inference-resnet50.yml | kubectl apply -f -\n</code></pre> <p>This is just a sample workload that compiles the torchvision ResNet50 model and runs repetitive inference in a loop to generate telemetry data.</p> <p>To verify the Pod was successfully deployed, please run:</p> <pre><code>kubectl get pods\n</code></pre> <p>You should see a pod named <code>pytorch-inference-resnet50</code>.</p> <p>After a few minutes, looking into the <code>Neuron / Monitor</code>, you should see the gathered metrics, similar to: </p> <p></p> <p></p> <p></p> <p></p> <p>Grafana Operator and Flux always work together to synchronize your dashboards with Git. If you delete your dashboards by accident, they will be re-provisioned automatically.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-inferentia-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-inferentia-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-istio-opensource-observability/","title":"Single Cluster Open Source Observability - Istio Monitoring","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-istio-opensource-observability/#objective","title":"Objective","text":"<p>Service Meshes are an integral part of the Kubernetes environment that enables secure, reliable, and observable communication. Istio is an open-source service mesh that provides advanced network features without requiring any changes to the application code. These capabilities include service-to-service authentication, monitoring, and more.</p> <p>Istio generates detailed telemetry for all service communications within a mesh. This telemetry provides observability of service behavior, thereby empowering operators to troubleshoot, maintain, and optimize their applications. These features don\u2019t impose additional burdens on service developers. To monitor service behavior, Istio generates metrics for all service traffic in, out, and within an Istio service mesh. These metrics provide information on behaviors, like traffic volume, traffic error rates, and request-response latency.</p> <p>In addition to monitoring the behavior of services within a mesh, it\u2019s essential to monitor the behavior of the mesh itself. Istio components export metrics which provides insights into the health and function of the mesh control plane.</p> <p>This pattern configures an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Istio as a service mesh,  Amazon Managed service for Prometheus, and Amazon Managed Grafana for monitoring your Istio Control and Data plane metrics</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-istio-opensource-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-istio-opensource-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern, except for step 7, where you need to replace \"context\" in <code>~/.cdk.json</code> with the following:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_ISTIO_CP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/v0.2.0/artifacts/grafana-dashboards/eks/istio/istio-control-plane-dashboard.json\",\n        \"GRAFANA_ISTIO_MESH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/v0.2.0/artifacts/grafana-dashboards/eks/istio/istio-mesh-dashboard.json\",\n        \"GRAFANA_ISTIO_PERF_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/v0.2.0/artifacts/grafana-dashboards/eks/istio/istio-performance-dashboard.json\",\n        \"GRAFANA_ISTIO_SERVICE_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/v0.2.0/artifacts/grafana-dashboards/eks/istio/istio-service-dashboard.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/istio\"\n        }\n      ]\n    },\n    \"Istio.pattern.enabled\": true\n  }\n</code></pre> <p>Once completed the rest of the Deploying steps, you can move on with the deployment of the Istio workload.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-istio-opensource-observability/#visualization","title":"Visualization","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-istio-opensource-observability/#1-grafana-dashboards","title":"1. Grafana dashboards","text":"<p>Go to the Dashboards panel of your Grafana workspace. You will see a list of Istio dashboards under the <code>Observability Accelerator Dashboards</code></p> <p></p> <p>Open one of the Istio dasbhoards and you will be able to view its visualization</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-istio-opensource-observability/#2-amazon-managed-service-for-prometheus-rules-and-alerts","title":"2. Amazon Managed Service for Prometheus rules and alerts","text":"<p>Open the Amazon Managed Service for Prometheus console and view the details of your workspace. Under the <code>Rules management</code> tab, you will find new rules deployed.</p> <p></p> <p>Note</p> <p>To setup your alert receiver, with Amazon SNS, follow this documentation</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-istio-opensource-observability/#deploy-an-example-application-to-visualize-metrics","title":"Deploy an example application to visualize metrics","text":"<p>In this section we will deploy Istio's Bookinfo sample application and extract metrics using the AWS OpenTelemetry collector. When downloading and configuring <code>istioctl</code>, there are samples included in the Istio package directory. The deployment files for Bookinfo are found in the <code>samples</code> folder. Additional details can be found on Istio's Getting Started documentation</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-istio-opensource-observability/#1-deploy-the-bookinfo-application","title":"1. Deploy the Bookinfo Application","text":"<ol> <li>Using the AWS CLI, configure kubectl so you can connect to your EKS cluster. Update for your region and EKS cluster name <pre><code>aws eks update-kubeconfig --region &lt;enter-your-region&gt; --name &lt;cluster-name&gt;\n</code></pre></li> <li>Label the default namespace for automatic Istio sidecar injection <pre><code>kubectl label namespace default istio-injection=enabled\n</code></pre></li> <li>Navigate to the Istio folder location. For example, if using Istio v1.18.2 in Downloads folder: <pre><code>cd ~/Downloads/istio-1.18.2\n</code></pre></li> <li>Deploy the Bookinfo sample application <pre><code>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\n</code></pre></li> <li>Connect the Bookinfo application with the Istio gateway <pre><code>kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml\n</code></pre></li> <li>Validate that there are no issues with the Istio configuration <pre><code>istioctl analyze\n</code></pre></li> <li>Get the DNS name of the load balancer for the Istio gateway <pre><code>GATEWAY_URL=$(kubectl get svc istio-ingressgateway -n istio-system -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n</code></pre></li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-istio-opensource-observability/#2-generate-traffic-for-the-istio-bookinfo-sample-application","title":"2. Generate traffic for the Istio Bookinfo sample application","text":"<p>For the Bookinfo sample application, visit <code>http://$GATEWAY_URL/productpage</code> in your web browser. To see trace data, you must send requests to your service. The number of requests depends on Istio\u2019s sampling rate and can be configured using the Telemetry API. With the default sampling rate of 1%, you need to send at least 100 requests before the first trace is visible. To send a 100 requests to the productpage service, use the following command: <pre><code>for i in $(seq 1 100); do curl -s -o /dev/null \"http://$GATEWAY_URL/productpage\"; done\n</code></pre></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-istio-opensource-observability/#3-explore-the-istio-dashboards","title":"3. Explore the Istio dashboards","text":"<p>Log back into your Amazon Managed Grafana workspace and navigate to the dashboard side panel. Click on the <code>Observability Accelerator Dashboards</code> folder and open the <code>Istio Service</code> Dashboard. Use the Service dropdown menu to select the <code>reviews.default.svc.cluster.local</code> service. This gives details about metrics for the service, client workloads (workloads that are calling this service), and service workloads (workloads that are providing this service).</p> <p></p> <p>Explore the Istio Control Plane, Mesh, and Performance dashboards as well.</p> <p>Control Plane  </p> <p></p> <p>Mesh  </p> <p>Performance</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-istio-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/","title":"Single Cluster Open Source Observability - Java Monitoring","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/#objective","title":"Objective","text":"<p>This pattern demonstrates how to use the New EKS Cluster Open Source Observability Accelerator with Java based workloads.</p> <p>It also enables control plane logging for comprehensive monitoring on the EKS cluster.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern, except for step 7, where you need to replace \"context\" in <code>~/.cdk.json</code> with the following:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_JAVA_JMX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/java/default.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/java\"\n        }\n      ]\n    },\n    \"java.pattern.enabled\": true\n  }\n</code></pre> <p>Once completed the rest of the Deploying steps, you can move on with the deployment of the Java workload.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/#deploy-an-example-java-application","title":"Deploy an example Java application","text":"<p>In this section we will reuse an example from the AWS OpenTelemetry collector repository. For convenience, the steps can be found below.</p> <ol> <li> <p>Clone this repository and navigate to the <code>sample-apps/jmx/</code> directory.</p> </li> <li> <p>Authenticate to Amazon ECR (AWS_REGION was set during the deployment stage)</p> </li> </ol> <pre><code>export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text`\naws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com\n</code></pre> <ol> <li>Create an Amazon ECR repository</li> </ol> <pre><code>aws ecr create-repository --repository-name prometheus-sample-tomcat-jmx \\\n  --image-scanning-configuration scanOnPush=true \\\n  --region $AWS_REGION \n</code></pre> <ol> <li>Build Docker image and push to ECR.</li> </ol> <pre><code>docker build -t $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/prometheus-sample-tomcat-jmx:latest .\ndocker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/prometheus-sample-tomcat-jmx:latest \n</code></pre> <ol> <li>Install the sample application in the cluster</li> </ol> <pre><code>SAMPLE_TRAFFIC_NAMESPACE=javajmx-sample\ncurl https://raw.githubusercontent.com/aws-observability/aws-otel-test-framework/terraform/sample-apps/jmx/examples/prometheus-metrics-sample.yaml | \nsed \"s/{{aws_account_id}}/$AWS_ACCOUNT_ID/g\" |\nsed \"s/{{region}}/$AWS_REGION/g\" |\nsed \"s/{{namespace}}/$SAMPLE_TRAFFIC_NAMESPACE/g\" | \nkubectl apply -f -\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<pre><code>kubectl get pods -n $SAMPLE_TRAFFIC_NAMESPACE\n\nNAME                              READY   STATUS    RESTARTS   AGE\ntomcat-bad-traffic-generator      1/1     Running   0          90m\ntomcat-example-77b46cc546-z22jf   1/1     Running   0          25m\ntomcat-traffic-generator          1/1     Running   0          90m\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/#visualization","title":"Visualization","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a new dashboard named <code>Java/JMX</code>, under <code>Observability Accelerator Dashboards</code>:</p> <p></p> <p>Open the <code>Java/JMX</code> dashboard and you should be able to view its visualization as shown below:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-java-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/","title":"Single Cluster AWS Mixed Observability","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Cluster Mixed Observability pattern using AWS native tools such as CloudWatch and X-Ray and Open Source tools such as AWS Distro for OpenTelemetry(ADOT) and Prometheus Node Exporter.</p> <p></p> <p>This example makes use of CloudWatch as a metric and log aggregation layer while X-Ray is used as a trace-aggregation layer. In order to collect the metrics and traces we use the Open Source ADOT collector. Fluent Bit is used to export the logs to CloudWatch Logs.</p> <p>In this architecture AWS X-Ray provides a complete view of requests as they travel through your application and filters visual data across payloads, functions, traces, services, and APIs. X-Ray also allows you to perform analytics to gain powerful insights about your distributed trace data.</p> <p>Utilizing CloudWatch and X-Ray as an aggregation layer allows for a fully-managed scalable telemetry backend. In this example we get those benefits while still having the flexibility and rapid development of the Open Source collection tools.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster.</li> <li>Enables Control Plane logging</li> <li>AWS Distro For OpenTelemetry Operator and Collector configured to collect metrics and traces.</li> <li>Logs with AWS for FluentBit and CloudWatch Logs</li> <li>Aggregate Metrics in CloudWatch</li> <li>Aggregate Traces in X-Ray</li> </ul> <p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository</p> </li> <li> <p>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</p> </li> </ol> <pre><code>make build\nmake pattern single-new-eks-mixed-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-mixed-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-opensource-singleneweksopensourceob-82N8N3BMJYYI\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <pre><code>kubectl get nodes -o wide\n</code></pre> <p>Output:</p> <pre><code>NAME                                         STATUS   ROLES    AGE    VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-144-134.us-west-1.compute.internal   Ready    &lt;none&gt;   143m   v1.25.9-eks-0a21954   10.0.144.134   &lt;none&gt;        Amazon Linux 2   5.10.179-168.710.amzn2.x86_64   containerd://1.6.19\n</code></pre> <p>Next, lets verify the namespaces in the cluster:</p> <pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <p>Output:</p> <pre><code>NAME                            STATUS   AGE\naws-for-fluent-bit              Active   142m\ncert-manager                    Active   142m\ndefault                         Active   148m\nexternal-secrets                Active   142m\nkube-node-lease                 Active   149m\nkube-public                     Active   149m\nkube-system                     Active   149m\nopentelemetry-operator-system   Active   142m\nprometheus-node-exporter        Active   142m\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/#visualization","title":"Visualization","text":"<p>Navigate to CloudWatch and go to Metrics -&gt; All Metrics.</p> <p>Select the metrics in the ContainerInsights/Prometheus Namespace:</p> <p></p> <p>View the graph of the selected metrics:</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/#viewing-logs","title":"Viewing Logs","text":"<p>Refer to \"Using CloudWatch Logs Insights to Query Logs in Logging.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-mixed-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-mixed-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/","title":"Single Cluster Open Source Observability - NGINX Monitoring","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/#objective","title":"Objective","text":"<p>This pattern demonstrates how to use the New EKS Cluster Open Source Observability Accelerator with Nginx based workloads.</p> <p>It also enables control plane logging to provide comprehensive overview of cluster health.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/#deploying","title":"Deploying","text":"<p>Please follow the Deploying instructions of the New EKS Cluster Open Source Observability Accelerator pattern, except for step 7, where you need to replace \"context\" in <code>~/.cdk.json</code> with the following:</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_NGINX_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/nginx/nginx.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        },\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/nginx\"\n        }\n      ]\n    },\n    \"nginx.pattern.enabled\": true\n  }\n</code></pre> <p>!! warning This scenario might need larger worker node for the pod. </p> <p>Once completed the rest of the Deploying steps, you can move on with the deployment of the Nginx workload.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/#deploy-an-example-nginx-application","title":"Deploy an example Nginx application","text":"<p>In this section we will deploy sample application and extract metrics using AWS OpenTelemetry collector.</p> <ol> <li> <p>Add NGINX ingress controller add-on into lib/single-new-eks-opensource-observability-pattern/index.ts in add-on array. <pre><code>        const addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n            new blueprints.addons.CloudWatchLogsAddon({\n                logGroupPrefix: `/aws/eks/${stackId}`,\n                logRetentionDays: 30\n            }),\n            new blueprints.addons.XrayAdotAddOn(),\n            new blueprints.addons.FluxCDAddOn({\"repositories\": [fluxRepository]}),\n            new GrafanaOperatorSecretAddon(),\n            new blueprints.addons.NginxAddOn({\n                name: \"ingress-nginx\",\n                chart: \"ingress-nginx\",\n                repository: \"https://kubernetes.github.io/ingress-nginx\",\n                version: \"4.7.2\",\n                namespace: \"nginx-ingress-sample\",\n                values: {\n                    controller: { \n                        metrics: {\n                            enabled: true,\n                            service: {\n                                annotations: {\n                                    \"prometheus.io/port\": \"10254\",\n                                    \"prometheus.io/scrape\": \"true\"\n                                }\n                            }\n                        }\n                    }\n                }\n            }),\n        ];\n</code></pre></p> </li> <li> <p>Deploy pattern again  <pre><code>make pattern single-new-eks-opensource-observability deploy\n</code></pre></p> </li> <li> <p>Verify if the application is running <pre><code>kubectl get pods -n nginx-ingress-sample\n</code></pre></p> </li> <li> <p>Set an EXTERNAL-IP variable to the value of the EXTERNAL-IP column in the row of the NGINX ingress controller. <pre><code>EXTERNAL_IP=$(kubectl get svc blueprints-addon-nginx-ingress-nginx-controller -n nginx-ingress-sample --output jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n</code></pre></p> </li> <li> <p>Start some sample NGINX traffic by entering the following command. <pre><code>SAMPLE_TRAFFIC_NAMESPACE=nginx-sample-traffic\ncurl https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/k8s-deployment-manifest-templates/nginx/nginx-traffic-sample.yaml |\nsed \"s/{{external_ip}}/$EXTERNAL_IP/g\" |\nsed \"s/{{namespace}}/$SAMPLE_TRAFFIC_NAMESPACE/g\" |\nkubectl apply -f -\n</code></pre></p> </li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<pre><code>kubectl get pod -n nginx-sample-traffic \n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/#visualization","title":"Visualization","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a new dashboard named <code>NGINX</code>, under <code>Observability Accelerator Dashboards</code>.</p> <p></p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-nginx-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/","title":"Single Cluster Open Source Observability","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#architecture","title":"Architecture","text":"<p>The following figure illustrates the architecture of the pattern we will be deploying for Single EKS Cluster Open Source Observability pattern using open source tooling such as AWS Distro for Open Telemetry (ADOT), Amazon Managed Service for Prometheus and Amazon Managed Grafana:</p> <p></p> <p>Monitoring Amazon Elastic Kubernetes Service (Amazon EKS) for metrics has two categories: the control plane and the Amazon EKS nodes (with Kubernetes objects). The Amazon EKS control plane consists of control plane nodes that run the Kubernetes software, such as etcd and the Kubernetes API server. To read more on the components of an Amazon EKS cluster, please read the service documentation.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#objective","title":"Objective","text":"<ul> <li>Deploys one production grade Amazon EKS cluster.</li> <li>Enables Control Plane logging.</li> <li>AWS Distro For OpenTelemetry Operator and Collector for Metrics and Traces</li> <li>Logs with AWS for FluentBit</li> <li>Installs Grafana Operator to add AWS data sources and create Grafana Dashboards to Amazon Managed Grafana.</li> <li>Installs FluxCD to perform GitOps sync of a Git Repo to EKS Cluster. We will use this later for creating Grafana Dashboards and AWS datasources to Amazon Managed Grafana. You can also use your own GitRepo  to sync your own Grafana resources such as Dashboards, Datasources etc. Please check our One observability module - GitOps with Amazon Managed Grafana to learn more about this.</li> <li>Installs External Secrets Operator to retrieve and Sync the Grafana API keys.</li> <li>Amazon Managed Grafana Dashboard and data source</li> <li>Alerts and recording rules with Amazon Managed Service for Prometheus</li> </ul>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#prerequisites","title":"Prerequisites:","text":"<p>Ensure that you have installed the following tools on your machine.</p> <ol> <li>aws cli</li> <li>kubectl</li> <li>cdk</li> <li>npm</li> </ol>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#deploying","title":"Deploying","text":"<ol> <li>Clone your forked repository</li> </ol> <pre><code>git clone https://github.com/aws-observability/cdk-aws-observability-accelerator.git\n</code></pre> <ol> <li>Install the AWS CDK Toolkit globally on your machine using</li> </ol> <pre><code>npm install -g aws-cdk\n</code></pre> <ol> <li>Amazon Managed Grafana workspace: To visualize metrics collected, you need an Amazon Managed Grafana workspace. If you have an existing workspace, create an environment variable as described below. To create a new workspace, visit our supporting example for Grafana</li> </ol> <p>Note</p> <p>For the URL <code>https://g-xyz.grafana-workspace.us-east-1.amazonaws.com</code>, the workspace ID would be <code>g-xyz</code></p> <pre><code>export AWS_REGION=&lt;YOUR AWS REGION&gt;\nexport COA_AMG_WORKSPACE_ID=g-xxx\nexport COA_AMG_ENDPOINT_URL=https://g-xyz.grafana-workspace.us-east-1.amazonaws.com\n</code></pre> <p>Warning</p> <p>Setting up environment variables <code>COA_AMG_ENDPOINT_URL</code> and <code>AWS_REGION</code> is mandatory for successful execution of this pattern.</p> <ol> <li>GRAFANA API KEY: Amazon Managed Grafana provides a control plane API for generating Grafana API keys or Service Account Tokens.</li> </ol> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ol> <li>AWS SSM Parameter Store for GRAFANA API KEY: Update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key. This will be referenced by Grafana Operator deployment of our solution to access Amazon Managed Grafana from Amazon EKS Cluster</li> </ol> <pre><code>aws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION\n</code></pre> <ol> <li> <p>Install project dependencies by running <code>npm install</code> in the main folder of this cloned repository. </p> </li> <li> <p>The actual settings for dashboard urls are expected to be specified in the CDK context. Generically it is inside the cdk.json file of the current directory or in <code>~/.cdk.json</code> in your home directory. </p> </li> </ol> <p>Example settings: Update the context in <code>cdk.json</code> file located in <code>cdk-eks-blueprints-patterns</code> directory</p> <pre><code>  \"context\": {\n    \"fluxRepository\": {\n      \"name\": \"grafana-dashboards\",\n      \"namespace\": \"grafana-operator\",\n      \"repository\": {\n        \"repoUrl\": \"https://github.com/aws-observability/aws-observability-accelerator\",\n        \"name\": \"grafana-dashboards\",\n        \"targetRevision\": \"main\",\n        \"path\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n      },\n      \"values\": {\n        \"GRAFANA_CLUSTER_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/cluster.json\",\n        \"GRAFANA_KUBELET_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kubelet.json\",\n        \"GRAFANA_NSWRKLDS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/namespace-workloads.json\",\n        \"GRAFANA_NODEEXP_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodeexporter-nodes.json\",\n        \"GRAFANA_NODES_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/nodes.json\",\n        \"GRAFANA_WORKLOADS_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/workloads.json\",\n        \"GRAFANA_KSH_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/ksh.json\",\n        \"GRAFANA_KCM_DASH_URL\" : \"https://raw.githubusercontent.com/aws-observability/aws-observability-accelerator/main/artifacts/grafana-dashboards/eks/infrastructure/kcm.json\"\n      },\n      \"kustomizations\": [\n        {\n          \"kustomizationPath\": \"./artifacts/grafana-operator-manifests/eks/infrastructure\"\n        }\n      ]\n    },\n  }\n</code></pre> <ol> <li>Once all pre-requisites are set you are ready to deploy the pipeline. Run the following command from the root of this repository to deploy the pipeline stack:</li> </ol> <pre><code>make build\nmake pattern single-new-eks-opensource-observability deploy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#verify-the-resources","title":"Verify the resources","text":"<p>Run update-kubeconfig command. You should be able to get the command from CDK output message.</p> <pre><code>aws eks update-kubeconfig --name single-new-eks-opensource-observability-accelerator --region &lt;your region&gt; --role-arn arn:aws:iam::xxxxxxxxx:role/single-new-eks-opensource-singleneweksopensourceob-82N8N3BMJYYI\n</code></pre> <p>Let\u2019s verify the resources created by steps above.</p> <p><pre><code>kubectl get nodes -o wide\n</code></pre> Output:</p> <pre><code>NAME                                         STATUS   ROLES    AGE    VERSION               INTERNAL-IP    EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME\nip-10-0-104-200.us-west-2.compute.internal   Ready    &lt;none&gt;   2d1h   v1.25.9-eks-0a21954   10.0.104.200   &lt;none&gt;        Amazon Linux 2   5.10.179-168.710.amzn2.x86_64   containerd://1.6.19\n</code></pre> <p>Next, lets verify the namespaces in the cluster:</p> <pre><code>kubectl get ns # Output shows all namespace\n</code></pre> <p>Output:</p> <pre><code>NAME                            STATUS   AGE\ncert-manager                    Active   2d1h\ndefault                         Active   2d1h\nexternal-secrets                Active   2d1h\nflux-system                     Active   2d1h\ngrafana-operator                Active   2d1h\nkube-node-lease                 Active   2d1h\nkube-public                     Active   2d1h\nkube-system                     Active   2d1h\nopentelemetry-operator-system   Active   2d1h\nprometheus-node-exporter        Active   2d1h\n</code></pre> <p>Next, lets verify all resources of <code>grafana-operator</code> namespace:</p> <pre><code>kubectl get all --namespace=grafana-operator\n</code></pre> <p>Output:</p> <pre><code>NAME                                    READY   STATUS    RESTARTS   AGE\npod/grafana-operator-866d4446bb-g5srl   1/1     Running   0          2d1h\n\nNAME                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/grafana-operator-metrics-service   ClusterIP   172.20.223.125   &lt;none&gt;        9090/TCP   2d1h\n\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grafana-operator   1/1     1            1           2d1h\n\nNAME                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grafana-operator-866d4446bb   1         1         1       2d1h\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#visualization","title":"Visualization","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#1-grafana-dashboards","title":"1. Grafana dashboards","text":"<p>Login to your Grafana workspace and navigate to the Dashboards panel. You should see a list of dashboards under the <code>Observability Accelerator Dashboards</code></p> <p></p> <p>Open the <code>Cluster</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Namespace (Workloads)</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Node (Pods)</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Workload</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Kubelet</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>Nodes</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>EKS Scheduler</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>Open the <code>EKS Control Manager</code> dashboard and you should be able to view its visualization as shown below :</p> <p></p> <p>From the cluster to view all dashboards as Kubernetes objects, run:</p> <pre><code>kubectl get grafanadashboards -A\n</code></pre> <pre><code>NAMESPACE          NAME                                   AGE\ngrafana-operator   cluster-grafanadashboard               138m\ngrafana-operator   java-grafanadashboard                  143m\ngrafana-operator   kubelet-grafanadashboard               13h\ngrafana-operator   namespace-workloads-grafanadashboard   13h\ngrafana-operator   nginx-grafanadashboard                 134m\ngrafana-operator   node-exporter-grafanadashboard         13h\ngrafana-operator   nodes-grafanadashboard                 13h\ngrafana-operator   workloads-grafanadashboard             13h\n</code></pre> <p>You can inspect more details per dashboard using this command</p> <pre><code>kubectl describe grafanadashboards cluster-grafanadashboard -n grafana-operator\n</code></pre> <p>Grafana Operator and Flux always work together to synchronize your dashboards with Git. If you delete your dashboards by accident, they will be re-provisioned automatically.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#viewing-logs","title":"Viewing Logs","text":"<p>Refer to the \"Using CloudWatch Logs as a data source in Grafana\" section in Logging.</p>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#teardown","title":"Teardown","text":"<p>You can teardown the whole CDK stack with the following command:</p> <pre><code>make pattern single-new-eks-opensource-observability destroy\n</code></pre>"},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#troubleshooting","title":"Troubleshooting","text":""},{"location":"patterns/single-new-eks-observability-accelerators/single-new-eks-opensource-observability/#1-grafana-dashboards-missing-or-grafana-api-key-expired","title":"1. Grafana dashboards missing or Grafana API key expired","text":"<p>In case you don't see the grafana dashboards in your Amazon Managed Grafana console, check on the logs on your grafana operator pod using the below command :</p> <pre><code>kubectl get pods -n grafana-operator\n</code></pre> <p>Output:</p> <pre><code>NAME                                READY   STATUS    RESTARTS   AGE\ngrafana-operator-866d4446bb-nqq5c   1/1     Running   0          3h17m\n</code></pre> <pre><code>kubectl logs grafana-operator-866d4446bb-nqq5c -n grafana-operator\n</code></pre> <p>Output:</p> <pre><code>1.6857285045556655e+09  ERROR   error reconciling datasource    {\"controller\": \"grafanadatasource\", \"controllerGroup\": \"grafana.integreatly.org\", \"controllerKind\": \"GrafanaDatasource\", \"GrafanaDatasource\": {\"name\":\"grafanadatasource-sample-amp\",\"namespace\":\"grafana-operator\"}, \"namespace\": \"grafana-operator\", \"name\": \"grafanadatasource-sample-amp\", \"reconcileID\": \"72cfd60c-a255-44a1-bfbd-88b0cbc4f90c\", \"datasource\": \"grafanadatasource-sample-amp\", \"grafana\": \"external-grafana\", \"error\": \"status: 401, body: {\\\"message\\\":\\\"Expired API key\\\"}\\n\"}\ngithub.com/grafana-operator/grafana-operator/controllers.(*GrafanaDatasourceReconciler).Reconcile\n</code></pre> <p>If you observe, the the above <code>grafana-api-key error</code> in the logs, your grafana API key is expired. Please use the operational procedure to update your <code>grafana-api-key</code> :</p> <ul> <li>First, lets create a new Grafana API key.</li> </ul> v10.4 &amp; v9.4 workspacesv8.4 workspaces <pre><code># IMPORTANT NOTE: skip this command if you already have a service token\nGRAFANA_SA_ID=$(aws grafana create-workspace-service-account \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --grafana-role ADMIN \\\n  --name cdk-accelerator-eks \\\n  --query 'id' \\\n  --output text)\n\n# creates a new token\nexport GO_AMG_API_KEY=$(aws grafana create-workspace-service-account-token \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  -name \"grafana-operator-key\" \\\n  --seconds-to-live 432000 \\\n  --service-account-id $GRAFANA_SA_ID \\\n  --query 'serviceAccountToken.key' \\\n  --output text)\n</code></pre> <pre><code>export GO_AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $COA_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n</code></pre> <ul> <li>Finally, update the Grafana API key secret in AWS SSM Parameter Store using the above new Grafana API key:</li> </ul> <pre><code>export API_KEY_SECRET_NAME=\"grafana-api-key\"\naws ssm put-parameter --name \"/cdk-accelerator/grafana-api-key\" \\\n    --type \"SecureString\" \\\n    --value $AMG_API_KEY \\\n    --region $AWS_REGION \\\n    --overwrite\n</code></pre> <ul> <li>If the issue persists, you can force the synchronization by deleting the <code>externalsecret</code> Kubernetes object.</li> </ul> <pre><code>kubectl delete externalsecret/external-secrets-sm -n grafana-operator\n</code></pre>"}]}